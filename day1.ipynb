{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f31350",
   "metadata": {},
   "source": [
    "# 注意力机制的基本逻辑\n",
    "## 向量之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e13dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "471ed4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50.,  68.],\n",
      "        [122., 167.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[16., 17., 18.],\n",
       "         [16., 17., 18.]]),\n",
       " tensor([[1.5230e-08, 1.0000e+00],\n",
       "         [2.8625e-20, 1.0000e+00]]),\n",
       " torch.Size([2, 2]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "q = torch.tensor(([1,2,3],[4,5,6]), dtype=torch.float32)  # 这是一个二维的 tensor， shape 是 [2, 3]\n",
    "k = torch.tensor(([7,8,9],[10,11,12]), dtype=torch.float32) # 这是一个二维的 tensor， shape 是 [2, 3]\n",
    "v = torch.tensor(([13,14,15],[16,17,18]), dtype=torch.float32) # 这是一个二维的 tensor， shape 是 [2, 3] \n",
    "# q,k,v 的 shape 必须是一样的，才能进行点积运算\n",
    "# 点积运算，实际上是矩阵乘法\n",
    "# q 和 k 进行点积运算，得到一个标量，然后这个标量再和 v 进行乘法运算，得到最终的结果 \n",
    "# 这个过程可以看作是 q 和 k 之间的相似度计算，然后根据相似度来加权 v 的值\n",
    "# 这个过程可以看作是注意力机制的核心思想\n",
    "# 具体实现可以使用 torch.matmul() 函数来进行矩阵乘法运算\n",
    "# 具体实现可以使用 torch.softmax() 函数来进行归一化处理\n",
    "attention = torch.matmul(q, k.T)  # q 和 k 进行点积运算，得到一个标量\n",
    "print(attention)  # attention 的 shape 是 [2, 2]\n",
    "attention = F.softmax(attention, dim=-1)  # 对点积结果进行归一化处理\n",
    "output = torch.matmul(attention, v)  # 将归一化结果和 v 进行乘法运算，得到最终的结果\n",
    "\n",
    "output, attention,attention.shape,v.shape,output.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb01daa",
   "metadata": {},
   "source": [
    "# softmax与q 的关系，\n",
    "softmax是针对 Attention 矩阵的每一行的，也就是，Attention 的每一行是每个input 向量与与其他向量的关联性，对行进行 softmax，将视角转换为 q，从 q 以统一的标准（softmax）去“看一下”其他向量，\n",
    "## 本质\n",
    "其实不是对 q 进行 softmax，而是对哪个进行 softmax，哪个就是 q，q 和 k 在进行 softmax 之前其实是对称的，q * kT 还是 q T* k 对称的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95021e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6784, 0.3216],\n",
       "         [0.2511, 0.7489],\n",
       "         [0.4085, 0.5915],\n",
       "         [0.3531, 0.6469]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([[[ 1.0994,  0.3531],\n",
    "          [-0.4143,  0.6784],\n",
    "          [ 0.0486,  0.4189],\n",
    "          [-0.8566, -0.2512]]])\n",
    "test = F.softmax(test, dim=-1)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0300e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "index-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
