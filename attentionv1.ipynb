{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41350fd",
   "metadata": {},
   "source": [
    "# 实现最基本的 Attention 机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0018a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 向量的点积（点乘）\n",
    "> torch.dot(x,y)跟 x @ y 是一样的，\n",
    "## torch.softmax()\n",
    "dim = 1, 向量的第n个维度上，1 就是第二个，如果是二维的话，1 和 -1 反而是一样的，\n",
    "dim = -1,向量的最后一个维度上\n",
    "\n",
    "tensor([[ 0.5793, -1.7151, -1.4405, -0.1477],\n",
    "        [-1.1456,  0.2692,  0.8947,  0.6108],\n",
    "        [-0.9580, -1.0135,  1.4974,  1.2203]])\n",
    "\n",
    "这是一个 3*4 的 tensor， shape 的话是 [3,4]\n",
    "\n",
    "\n",
    "tensor([[[ 1.0994,  0.3531],\n",
    "         [-1.1037, -1.0341],\n",
    "         [ 1.0761,  1.0038],\n",
    "         [-0.5333,  0.5775]],\n",
    "\n",
    "        [[ 1.5154, -1.2769],\n",
    "         [ 0.2073,  0.4081],\n",
    "         [-1.3587,  1.0832],\n",
    "         [-1.4338,  0.1262]],\n",
    "\n",
    "        [[-0.5507, -0.6281],\n",
    "         [-0.7851,  1.4680],\n",
    "         [ 0.0486,  0.4189],\n",
    "         [-0.8566, -0.2512]]])\n",
    "\n",
    "这是一个 [3,4,2]的 tensor，实际打印出来是反过来的，最里面的单元是2 个浮点数组成的，4 个这样的组合，组成第二层单元，这样的单元，有3 个，\n",
    "从里到外的一个过程，这一点在后续处理各种 tensor 很重要，要对 tensor 有这种基本的感觉，以后各种数据一来就知道是什么 shape，对数据的 shape 要非常敏感\n",
    "整个神经网络模型，实际上并不是网络，实际是各种 shape 的转换，神经网络只是一种说法，用来糊弄糊弄外行的，实际机器学习就是玩各种 shape，跟 GPU 打交道也是 shape，参数也是 shape，根本不存在像神经一样的网络，基本不沾什么边，\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff426bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5060,  0.3686,  1.1199],\n",
       "        [-2.6047,  0.3158, -0.7354],\n",
       "        [ 0.1548, -0.4813, -0.3124],\n",
       "        [ 0.0210,  1.2137,  1.6285],\n",
       "        [ 0.3561, -0.1072,  1.3742],\n",
       "        [-1.2743,  0.8822,  0.8836]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test1 = torch.randn(6, 3)\n",
    "test1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ccaa9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.8547,  -0.8081,  -0.3946,   1.4745,   0.8547,   0.6038],\n",
       "         [ -0.9138, -11.5930,   4.8040, -10.0269,  -0.9138, -11.4487],\n",
       "         [ -0.1433,   2.2329,  -0.6218,   1.0937,  -0.1433,   1.6883],\n",
       "         [  1.0512,  -5.6924,   1.0857,  -1.2394,   1.0512,  -3.2838],\n",
       "         [  0.8547,  -0.8081,  -0.3946,   1.4745,   0.8547,   0.6038],\n",
       "         [  0.3857,  -9.8191,   3.1184,  -5.6999,   0.3857,  -7.9094]],\n",
       "        device='mps:0', grad_fn=<DivBackward0>),\n",
       " tensor([[1.9558e-01, 3.7083e-02, 5.6072e-02, 3.6351e-01, 1.9558e-01, 1.5218e-01],\n",
       "         [3.2655e-03, 7.5169e-08, 9.9347e-01, 3.5988e-07, 3.2655e-03, 8.6836e-08],\n",
       "         [4.3339e-02, 4.6653e-01, 2.6859e-02, 1.4932e-01, 4.3339e-02, 2.7061e-01],\n",
       "         [3.1741e-01, 3.7401e-04, 3.2852e-01, 3.2123e-02, 3.1741e-01, 4.1583e-03],\n",
       "         [1.9558e-01, 3.7083e-02, 5.6072e-02, 3.6351e-01, 1.9558e-01, 1.5218e-01],\n",
       "         [5.7550e-02, 2.1289e-06, 8.8475e-01, 1.3095e-04, 5.7550e-02, 1.4372e-05]],\n",
       "        device='mps:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.tensor([[ 0.5060,  0.3686,  1.1199],\n",
    "        [-2.6047,  0.3158, -0.7354],\n",
    "        [ 0.1548, -0.4813, -0.3124],\n",
    "        [ 0.0210,  1.2137,  1.6285],\n",
    "        [ 0.5060,  0.3686,  1.1199],  #重复的行\n",
    "        [-1.2743,  0.8822,  0.8836]])\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "input = input.to(device)\n",
    "\n",
    "input_d = input.shape[-1]# 获取最后一个维度的大小, 代表每个 token 的维度，就是所谓的d_model\n",
    "#Wq需要将 input 从 d_model 维度映射到 d_k 维度，当然是在实际应用中，都是 d_k < d_model的，比如从 7168 降为 128 之类，\n",
    "d_k = 4  #假设我们要把维度映射到4维\n",
    "Wq = torch.nn.Parameter(torch.randn(input_d, d_k), requires_grad=True).to(device)  #权重矩阵，维度是 [d_model, d_k]\n",
    "Wk = torch.nn.Parameter(torch.randn(input_d, d_k), requires_grad=True).to(device)  #权重矩阵，维度是 [d_model, d_k]\n",
    "Wv = torch.nn.Parameter(torch.randn(input_d, d_k), requires_grad=True).to(device)  #权重矩阵，维度是 [d_model, d_k]\n",
    "\n",
    "Q = torch.matmul(input, Wq)  #计算 Q 矩阵，维度是 [seq_len, d_k]\n",
    "K = torch.matmul(input, Wk)  #计算 K 矩阵，维度是 [seq_len, d_k]\n",
    "V = torch.matmul(input, Wv)  #计算 V 矩阵，维度是 [seq_len, d_k]\n",
    "\n",
    "atten_scores = torch.matmul(Q, K.T) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))  #计算注意力分数，维度是 [seq_len, seq_len]\n",
    "\n",
    "\n",
    "atten_weights = torch.softmax(atten_scores, dim=-1)  #计算注意力权重，维度是 [seq_len, seq_len]\n",
    "\n",
    "\n",
    "atten_scores, atten_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a162158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d84acc09",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 测试是否在 device 上运行，运行 10000 次 看看时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "for _ in range(1000000):\n",
    "    Q = torch.matmul(input, Wq)  #计算 Q 矩阵，维度是 [seq_len, d_k]\n",
    "    K = torch.matmul(input, Wk)  #计算 K 矩阵，维度是 [seq_len, d_k]\n",
    "    V = torch.matmul(input, Wv)  #计算 V 矩阵，维度是 [seq_len, d_k]\n",
    "    atten_scores = torch.matmul(Q, K.T) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))  #计算注意力分数，维度是 [seq_len, seq_len]\n",
    "    atten_weights = torch.softmax(atten_scores, dim=-1)  #计算注意力权重，维度是 [seq_len, seq_len]\n",
    "    output = torch.matmul(atten_weights, V)  #计算输出，维度是 [seq_len, d_k]       \n",
    "end_time = time.time()\n",
    "print(f\"Time taken for 1000000 iterations: {end_time - start_time} seconds\")\n",
    "\n",
    "# 现在还没有位置编码，所以经过 Attention 处理以后，有两个值还是是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c142f71",
   "metadata": {},
   "source": [
    "# 点积与矩阵乘法\n",
    "## 矩阵乘法，[a,x]必须与[x,b]的矩阵相乘，否则就不能相乘，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f5ea508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(32.), tensor(32.))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(6, 3)\n",
    "b = torch.randn(3, 4)\n",
    "c = torch.matmul(a, b)\n",
    "c  # 这是一个 6*4 的矩阵，矩阵乘法是a的每一行，与 b 的每一列做点积运算，得到 c 的每一个元素\n",
    "# 点积是两个向量对应元素相乘再求和，矩阵乘法是多个点积组成的，这就是点积与矩阵乘法的关系\n",
    "\n",
    "## 向量的点积（点乘）\n",
    "# torch.dot(x,y)跟 x @ y 是一样的，\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "dot_product1 = torch.dot(a, b)  # 使用 torch.dot 计算点积\n",
    "dot_product2 = a @ b  # 使用 @ 运算符计算点积\n",
    "dot_product1, dot_product2  # 两种方法的结果是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ee1af23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), tensor([0.5000, 1.0000, 1.5000]), tensor(6.))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,a/2,a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "530eb534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7778), tensor(1.1738), tensor(0.9758))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = torch.tensor([0.4, 1.4])\n",
    "k1 = torch.tensor([0.3, 0.7])\n",
    "k2 = torch.tensor([0.3, 1.1])\n",
    "k3 = torch.tensor([0.3, 0.9])\n",
    "\n",
    "d_model = 2\n",
    "\n",
    "atten_scores1 = torch.dot(q2, k1) / (2**0.5)\n",
    "atten_scores2 = torch.dot(q2, k2) / 2**0.5  \n",
    "atten_scores3 = torch.dot(q2, k3) / 2**0.5\n",
    "\n",
    "atten_scores1, atten_scores2, atten_scores3\n",
    "\n",
    "\n",
    "                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "index-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
