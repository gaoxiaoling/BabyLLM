{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951db78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 GPTV.py 中的 GPTModel 类进行训练，训练数据是 the-verdict.txt\n",
    "# 包括两个部分\n",
    "# 第一部分，数据处理，将 the-verdict.txt 中的文本转换为训练和测试两个数据集\n",
    "# 第二部分，模型训练，使用训练数据集训练模型，并实时跟踪在测试集上的效果\n",
    "\n",
    "import torch\n",
    "#\n",
    "from GPTV1 import GPTModel\n",
    "import tiktoken\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c3df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100277\n",
      "[271, 17905, 11239, 115, 51431, 15120, 18904, 220, 220, 7518, 226, 45826, 104, 37795, 238, 162, 95, 99, 7190, 119, 6744, 228, 33035, 66285, 113, 23249, 13647, 122, 25132, 101, 14276, 239, 72406, 236, 16175, 246, 17486, 222, 13357, 118, 14191, 222, 271, 220, 73028, 97, 30867, 11239, 115, 30537, 15120, 18904, 75863, 59983, 58521, 37026, 6823, 239, 5232, 63212, 27552, 122, 82302, 39282, 15120, 87217, 162, 95, 99, 7190, 119, 55030, 34547, 3922, 8067, 227, 45163, 89151, 30926, 37795, 238, 86436, 3922, 69636, 20022, 253, 1, 33035, 66285, 113, 1, 55030, 37687, 3922, 58843, 108, 33091, 28038, 163, 59815]\n",
      "1190621\n",
      "sample_tokens:  [20119, 245, 30868, 249, 29207, 231, 24326, 241, 8687, 237, 8676, 251, 29207, 231, 9554, 46034, 37687]\n",
      "1071558\n",
      "119063\n",
      "训练数据集样本数量:  4185\n",
      "测试数据集样本数量:  465\n",
      "['model_epoch_2.pt', 'model_epoch_1.pt', 'model_epoch_0.pt']\n",
      "Loaded model from ./model_epoch_2.pt\n",
      "Start training from epoch 2\n",
      "Total number of parameters: 232163328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\" : 100277,\n",
    "\"context_length\" : 256,\n",
    "\"emb_dim\" : 768,\n",
    "\"n_heads\" : 12,\n",
    "\"n_layers\" : 12,\n",
    "\"drop_rate\" : 0.1,\n",
    "\"qkv_bias\" : False\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "# GPT_CONFIG_124M = {\n",
    "#     \"vocab_size\": 50257,     # Vocabulary size\n",
    "#     \"context_length\": 1024,  # Context length\n",
    "#     \"emb_dim\": 768,          # Embedding dimension\n",
    "#     \"n_heads\": 12,           # Number of attention heads\n",
    "#     \"n_layers\": 12,          # Number of layers\n",
    "#     \"drop_rate\": 0.1,        # Dropout rate\n",
    "#     \"qkv_bias\": False        # Query-Key-Value bias\n",
    "# }\n",
    "\n",
    "\n",
    "# # 数据处理\n",
    "# # 加载数据文件，the-verdict.txt\n",
    "# with open(\"./LLM/the-verdict.txt\", \"r\") as f:\n",
    "#     data = f.read()\n",
    "# print(data[:100])\n",
    "\n",
    "\n",
    "# 加载四个txt文件的数据作为训练数据，./data/12.txt, ./data/3.txt, ./data/108.txt, ./data/4.txt\n",
    "# 每个文件中的数据量为 1000 条\n",
    "# 每个文件中的数据量为 1000 条\n",
    "with open(\"./data/12.txt\", \"r\") as f:\n",
    "    data12 = f.read()\n",
    "# with open(\"./data/3.txt\", \"r\") as f:\n",
    "#     data3 = f.read()\n",
    "# with open(\"./data/108.txt\", \"r\") as f:\n",
    "#     data108 = f.read()\n",
    "# with open(\"./data/4.txt\", \"r\") as f:\n",
    "#     data4 = f.read()\n",
    "\n",
    "# 合并四个文件的数据\n",
    "data = data12 #+ data3 + data108 + data4 \n",
    "\n",
    "\n",
    "# 将数据转换为 tokens\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# 词典大小\n",
    "print(tokenizer.n_vocab)\n",
    "#GPT_CONFIG_124M[\"vocab_size\"] = tokenizer.n_vocab\n",
    "\n",
    "#tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = tokenizer.encode(data)\n",
    "print(tokens[:100])\n",
    "print(len(tokens))\n",
    "\n",
    "\n",
    "\n",
    "# 以一个具体例子直观测试模型表现\n",
    "# 从文件 the-verdict.txt中取一段\n",
    "# I looked about the spacious white-panelled room, with its _famille-verte_ vases repeating the tones of the pale damask curtains, and its eighteenth-century pastels in delicate faded frames.\n",
    "# 让模型续写“I looked about the spacious”\n",
    "sample_tokens = tokenizer.encode(\"林黛玉抓住宝玉的手说\") \n",
    "print(\"sample_tokens: \", sample_tokens) \n",
    "\n",
    "\n",
    "# 封装dataset 和dataloader,每个样本的max_len为 256，batch_size 为 2\n",
    "# 比如第一个样本为，0-255 个 token，其对应的标签为 1-256 个 token，以此类推\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, tokens, max_len, stride):\n",
    "        self.tokens = tokens\n",
    "        self.max_len = max_len\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - self.max_len) // self.stride + 1 # 每个样本的长度为 max_len，每个样本之间的步长为 stride\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.stride\n",
    "        end = start + self.max_len\n",
    "        input_ids = self.tokens[start:end]\n",
    "        label_ids = self.tokens[start+1:end+1]\n",
    "        return torch.tensor(input_ids), torch.tensor(label_ids)\n",
    "\n",
    "# check dataset and dataloader\n",
    "# 默认按照context_length进行切分\n",
    "max_len = GPT_CONFIG_124M[\"context_length\"]\n",
    "stride = max_len # 每个样本之间的步长为 128，即每个样本的 token 数量为 256，每个样本的标签 token 数量为 255\n",
    "# 如果stride=max_len，那么每个样本之间就没有重叠，每个样本都是独立的\n",
    "# 如果stride<max_len，那么每个样本之间就会有重叠，每个样本的 token 数量会小于 max_len\n",
    "\n",
    "train_tokens = tokens[:int(0.9*len(tokens))]\n",
    "test_tokens = tokens[int(0.9*len(tokens)):] # 测试数据集使用最后 10% 的 token\n",
    "\n",
    "print(len(train_tokens))\n",
    "print(len(test_tokens))\n",
    "\n",
    "train_dataset = GPTDatasetV1(train_tokens, max_len, stride)\n",
    "test_dataset = GPTDatasetV1(test_tokens, max_len, stride)\n",
    "\n",
    "print(\"训练数据集样本数量: \", len(train_dataset))\n",
    "print(\"测试数据集样本数量: \", len(test_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# for batch in train_dataloader:\n",
    "#     input_ids, label_ids = batch\n",
    "#     print(\"input_ids shape, label_ids shape: \", input_ids.shape, label_ids.shape)\n",
    "\n",
    "# 训练过程分多个 epoch, 每个 epoch 训练完所有的样本, 每个 epoch 结束后, 用测试数据集评估模型效果\n",
    "# 每个 epoch 中包含多个 batch，针对每个 batch 进行前向传播、计算损失、反向传播、更新参数\n",
    "# 每个 batch 训练完成后，用测试数据集评估模型效果\n",
    "\n",
    "# 初始化记录训练过程和训练效果的变量\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# 定时将训练损失和测试损失记录下来，用于后续可视化分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc400f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import必要的包\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import os\n",
    "\n",
    "\n",
    "# 加载模型，设置必要的环境和参数\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# 检查是否存在checkpoint文件，如 model_epoch_11.pt\n",
    "# 读取当前目录下所有的 checkpoint 文件\n",
    "checkpoint_path = \"./\"\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_path) if f.startswith(\"model_epoch_\") and f.endswith(\".pt\")]\n",
    "print(checkpoint_files)\n",
    "# 找到最新的 checkpoint 文件\n",
    "latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])) # 找到最新的 checkpoint 文件，根据 epoch 序号排序\n",
    "checkpoint_path = checkpoint_path + latest_checkpoint\n",
    "epoch_str = checkpoint_path.split(\"_\")[-1].split(\".\")[0]\n",
    "start_epoch = int(epoch_str)\n",
    "\n",
    "# 加载模型参数\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded model from {checkpoint_path}\")\n",
    "\n",
    "# 读取 epoch 的序号，从文件名中提取\n",
    "# 从文件名中提取 epoch 序号\n",
    "epoch_str = checkpoint_path.split(\"_\")[-1].split(\".\")[0]\n",
    "start_epoch = int(epoch_str)\n",
    "print(f\"Start training from epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "\n",
    "# 统计模型参数数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# 初始化优化器和学习率调度器\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "# 使用 adamw 优化器\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)    \n",
    "#scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.988 ** epoch) # 学习率衰减策略，每个 epoch 学习率衰减为原来的 98%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用模型在输入上连续生成50个字\n",
    "def generate_text(model, sample_tokens, max_new_tokens=100):\n",
    "    model.eval() # 切换到评估模式\n",
    "    #copy sample_tokens to avoid modifying the original list\n",
    "    sample_tokens = sample_tokens.copy()\n",
    "\n",
    "    with torch.no_grad(): # 关闭梯度计算\n",
    "        for _ in range(max_new_tokens):\n",
    "            input_ids = torch.tensor([sample_tokens]).to(device) # 输入测试数据集\n",
    "            sample_output = model(input_ids) # 生成 1 个新 token, output shape: (1, seq_len, vocab_size)\n",
    "\n",
    "            # 取输出中最后一个 token 的 logits\n",
    "            sample_logits = sample_output[0, -1, :] # 取输出中最后一个 token 的 logits, 形状为 (vocab_size,)\n",
    "            # # 取 logits 中最大的那个 token 作为生成的 token\n",
    "            # new_token = torch.argmax(sample_logits, dim=-1).item()\n",
    "\n",
    "            # 不是直接选择最大值对应的 token id，\n",
    "            # 使用 top-k算法\n",
    "            # 取 logits 中最大的 k 个 token 作为生成的 token\n",
    "            k = 3\n",
    "            top_k_logits, top_k_indices = torch.topk(sample_logits, k=k) # logits是具体的值，indices 是对应的位置\n",
    "            # 从 top k 个 token 中随机采样一个作为生成的 token\n",
    "            # 不是简单的随机的，要考虑top-k中每个token的概率, 概率越高，被选中的概率就越大\n",
    "            # 计算每个 token 被选中的概率\n",
    "            probs = torch.softmax(top_k_logits, dim=-1)\n",
    "            \n",
    "            # 从 top k 个 token 中随机采样一个作为生成的 token\n",
    "            new_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            # 获取对应的原始 位置 Id\n",
    "            new_token = top_k_indices[new_token].item() # 从 top k 个 token 中随机采样一个作为生成的 token\n",
    "\n",
    "            sample_tokens.append(new_token) # 将生成的 token 加入到输入中    \n",
    "    return sample_tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_tokens = tokenizer.encode(\"你好\")\n",
    "print(\"sample_tokens: \", sample_tokens)\n",
    "#验证模型在sample_tokens上的表现\n",
    "model.eval() # 切换到评估模式\n",
    "with torch.no_grad(): # 关闭梯度计算\n",
    "    output = generate_text(model, sample_tokens, max_new_tokens=100) \n",
    "    print(\"output tokens: \", len(output))\n",
    "    print(\"generated text: \", tokenizer.decode(output)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40883c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Batch 1\n",
      "train loss:  2.5602993965148926\n",
      "test loss:  3.217411176363627\n",
      "Batch 2\n",
      "train loss:  2.390638828277588\n",
      "test loss:  3.2245121955871583\n",
      "Batch 3\n",
      "train loss:  2.5045166015625\n",
      "test loss:  3.233021132151286\n",
      "Batch 4\n",
      "train loss:  2.4330620765686035\n",
      "test loss:  3.2358378489812214\n",
      "Batch 5\n",
      "train loss:  2.7783761024475098\n",
      "test loss:  3.2277239561080933\n",
      "Batch 6\n",
      "train loss:  2.599982738494873\n",
      "test loss:  3.2198904514312745\n",
      "Batch 7\n",
      "train loss:  2.6561965942382812\n",
      "test loss:  3.220629111925761\n",
      "Batch 8\n",
      "train loss:  2.717263698577881\n",
      "test loss:  3.222316869099935\n",
      "Batch 9\n",
      "train loss:  2.708259105682373\n",
      "test loss:  3.222512729962667\n",
      "Batch 10\n",
      "train loss:  2.6433258056640625\n",
      "test loss:  3.225003735224406\n",
      "Batch 11\n",
      "train loss:  2.8331809043884277\n",
      "test loss:  3.222511132558187\n",
      "Batch 12\n",
      "train loss:  2.5084707736968994\n",
      "test loss:  3.224675496419271\n",
      "Batch 13\n",
      "train loss:  2.4547603130340576\n",
      "test loss:  3.2225073178609214\n",
      "Batch 14\n",
      "train loss:  2.715313196182251\n",
      "test loss:  3.2238702217737836\n",
      "Batch 15\n",
      "train loss:  2.374795436859131\n",
      "test loss:  3.2242351531982423\n",
      "Batch 16\n",
      "train loss:  2.3637375831604004\n",
      "test loss:  3.227600367863973\n",
      "Batch 17\n",
      "train loss:  2.583989143371582\n",
      "test loss:  3.234576416015625\n",
      "Batch 18\n",
      "train loss:  2.525482654571533\n",
      "test loss:  3.234746567408244\n",
      "Batch 19\n",
      "train loss:  2.525367259979248\n",
      "test loss:  3.2354536851247153\n",
      "Batch 20\n",
      "train loss:  2.570906639099121\n",
      "test loss:  3.230416258176168\n",
      "Batch 21\n",
      "train loss:  2.5493907928466797\n",
      "test loss:  3.2255045890808107\n",
      "Batch 22\n",
      "train loss:  2.650280475616455\n",
      "test loss:  3.2290518363316854\n",
      "Batch 23\n",
      "train loss:  2.8258113861083984\n",
      "test loss:  3.238015604019165\n",
      "Batch 24\n",
      "train loss:  2.5454940795898438\n",
      "test loss:  3.2448013623555503\n",
      "Batch 25\n",
      "train loss:  2.6754183769226074\n",
      "test loss:  3.2432663758595783\n",
      "Batch 26\n",
      "train loss:  2.6489315032958984\n",
      "test loss:  3.2333709716796877\n",
      "Batch 27\n",
      "train loss:  2.6250667572021484\n",
      "test loss:  3.229025665918986\n",
      "Batch 28\n",
      "train loss:  2.5187838077545166\n",
      "test loss:  3.234544118245443\n",
      "Batch 29\n",
      "train loss:  2.6284518241882324\n",
      "test loss:  3.237892254193624\n",
      "Batch 30\n",
      "train loss:  2.646183729171753\n",
      "test loss:  3.246517125765483\n",
      "Batch 31\n",
      "train loss:  2.5410308837890625\n",
      "test loss:  3.2483999331792197\n",
      "Batch 32\n",
      "train loss:  2.4696993827819824\n",
      "test loss:  3.2476067781448363\n",
      "Batch 33\n",
      "train loss:  2.6318907737731934\n",
      "test loss:  3.242496053377787\n",
      "Batch 34\n",
      "train loss:  2.6889243125915527\n",
      "test loss:  3.238633330663045\n",
      "Batch 35\n",
      "train loss:  2.3898887634277344\n",
      "test loss:  3.2425636927286785\n",
      "Batch 36\n",
      "train loss:  2.759894371032715\n",
      "test loss:  3.2466803948084513\n",
      "Batch 37\n",
      "train loss:  2.7355732917785645\n",
      "test loss:  3.243074893951416\n",
      "Batch 38\n",
      "train loss:  2.578640937805176\n",
      "test loss:  3.241506989796956\n",
      "Batch 39\n",
      "train loss:  2.4359843730926514\n",
      "test loss:  3.242719276746114\n",
      "Batch 40\n",
      "train loss:  2.575852870941162\n",
      "test loss:  3.2487260739008588\n",
      "Batch 41\n",
      "train loss:  2.628512382507324\n",
      "test loss:  3.2470637639363606\n",
      "Batch 42\n",
      "train loss:  2.4512064456939697\n",
      "test loss:  3.24837642510732\n",
      "Batch 43\n",
      "train loss:  2.5272226333618164\n",
      "test loss:  3.2483644008636476\n",
      "Batch 44\n",
      "train loss:  2.5226049423217773\n",
      "test loss:  3.2482018947601317\n",
      "Batch 45\n",
      "train loss:  2.6128034591674805\n",
      "test loss:  3.247103238105774\n",
      "Batch 46\n",
      "train loss:  2.5925216674804688\n",
      "test loss:  3.2407012303670246\n",
      "Batch 47\n",
      "train loss:  2.486909866333008\n",
      "test loss:  3.2398792028427126\n",
      "Batch 48\n",
      "train loss:  2.686729907989502\n",
      "test loss:  3.244910399119059\n",
      "Batch 49\n",
      "train loss:  2.4921231269836426\n",
      "test loss:  3.245389437675476\n",
      "Batch 50\n",
      "train loss:  2.3029582500457764\n",
      "test loss:  3.2515899737675986\n",
      "Batch 51\n",
      "train loss:  2.598841667175293\n",
      "test loss:  3.2562821388244627\n",
      "Batch 52\n",
      "train loss:  2.5839426517486572\n",
      "test loss:  3.2539813121159873\n",
      "Batch 53\n",
      "train loss:  2.6530001163482666\n",
      "test loss:  3.2443742911020914\n",
      "Batch 54\n",
      "train loss:  2.5366244316101074\n",
      "test loss:  3.2423421780268353\n",
      "Batch 55\n",
      "train loss:  2.5831046104431152\n",
      "test loss:  3.2462523937225343\n",
      "Batch 56\n",
      "train loss:  2.662649631500244\n",
      "test loss:  3.252060906092326\n",
      "Batch 57\n",
      "train loss:  2.7051901817321777\n",
      "test loss:  3.2534966389338176\n",
      "Batch 58\n",
      "train loss:  2.585843086242676\n",
      "test loss:  3.2510167996088666\n",
      "Batch 59\n",
      "train loss:  2.629729986190796\n",
      "test loss:  3.246229553222656\n",
      "Batch 60\n",
      "train loss:  2.578521251678467\n",
      "test loss:  3.242260479927063\n",
      "Batch 61\n",
      "train loss:  2.578603506088257\n",
      "test loss:  3.241074792544047\n",
      "Batch 62\n",
      "train loss:  2.5161354541778564\n",
      "test loss:  3.2454903523127236\n",
      "Batch 63\n",
      "train loss:  2.5865468978881836\n",
      "test loss:  3.2503858248392743\n",
      "Batch 64\n",
      "train loss:  2.6681246757507324\n",
      "test loss:  3.25269197622935\n",
      "Batch 65\n",
      "train loss:  2.515873432159424\n",
      "test loss:  3.2497804959615073\n",
      "Batch 66\n",
      "train loss:  2.810054063796997\n",
      "test loss:  3.24499142964681\n",
      "Batch 67\n",
      "train loss:  2.6224205493927\n",
      "test loss:  3.240979218482971\n",
      "Batch 68\n",
      "train loss:  2.6302261352539062\n",
      "test loss:  3.2431204954783124\n",
      "Batch 69\n",
      "train loss:  2.549191951751709\n",
      "test loss:  3.2500020345052083\n",
      "Batch 70\n",
      "train loss:  2.662248134613037\n",
      "test loss:  3.2525590896606444\n",
      "Batch 71\n",
      "train loss:  2.8080408573150635\n",
      "test loss:  3.250496673583984\n",
      "Batch 72\n",
      "train loss:  2.457508087158203\n",
      "test loss:  3.2458034912745157\n",
      "Batch 73\n",
      "train loss:  2.6604819297790527\n",
      "test loss:  3.2471225102742514\n",
      "Batch 74\n",
      "train loss:  2.6820156574249268\n",
      "test loss:  3.2465429464975992\n",
      "Batch 75\n",
      "train loss:  2.402057647705078\n",
      "test loss:  3.2490657250086468\n",
      "Batch 76\n",
      "train loss:  2.6643271446228027\n",
      "test loss:  3.255911612510681\n",
      "Batch 77\n",
      "train loss:  2.589163064956665\n",
      "test loss:  3.2533252875010175\n",
      "Batch 78\n",
      "train loss:  2.6014928817749023\n",
      "test loss:  3.245888376235962\n",
      "Batch 79\n",
      "train loss:  2.642697811126709\n",
      "test loss:  3.243258515993754\n",
      "Batch 80\n",
      "train loss:  2.6805920600891113\n",
      "test loss:  3.2436912854512534\n",
      "Batch 81\n",
      "train loss:  2.6951210498809814\n",
      "test loss:  3.2496297041575115\n",
      "Batch 82\n",
      "train loss:  2.6431801319122314\n",
      "test loss:  3.2563318649927777\n",
      "Batch 83\n",
      "train loss:  2.5943777561187744\n",
      "test loss:  3.261469006538391\n",
      "Batch 84\n",
      "train loss:  2.5584816932678223\n",
      "test loss:  3.2587817668914796\n",
      "Batch 85\n",
      "train loss:  2.7303178310394287\n",
      "test loss:  3.2497373183568317\n",
      "Batch 86\n",
      "train loss:  2.51511812210083\n",
      "test loss:  3.2451751629511514\n",
      "Batch 87\n",
      "train loss:  2.596623420715332\n",
      "test loss:  3.254199163118998\n",
      "Batch 88\n",
      "train loss:  2.6358492374420166\n",
      "test loss:  3.255744528770447\n",
      "Batch 89\n",
      "train loss:  2.5584094524383545\n",
      "test loss:  3.2559537728627523\n",
      "Batch 90\n",
      "train loss:  2.609808921813965\n",
      "test loss:  3.25606742699941\n",
      "Batch 91\n",
      "train loss:  2.5981457233428955\n",
      "test loss:  3.2520544449488322\n",
      "Batch 92\n",
      "train loss:  2.5219905376434326\n",
      "test loss:  3.2522499243418377\n",
      "Batch 93\n",
      "train loss:  2.639493465423584\n",
      "test loss:  3.254815189043681\n",
      "Batch 94\n",
      "train loss:  2.7249860763549805\n",
      "test loss:  3.256346662839254\n",
      "Batch 95\n",
      "train loss:  2.5469820499420166\n",
      "test loss:  3.2502790451049806\n",
      "Batch 96\n",
      "train loss:  2.736905813217163\n",
      "test loss:  3.251074703534444\n",
      "Batch 97\n",
      "train loss:  2.6239383220672607\n",
      "test loss:  3.249735474586487\n",
      "Batch 98\n",
      "train loss:  2.705268383026123\n",
      "test loss:  3.241254281997681\n",
      "Batch 99\n",
      "train loss:  2.676269292831421\n",
      "test loss:  3.242581009864807\n",
      "Batch 100\n",
      "train loss:  2.534599781036377\n",
      "test loss:  3.2469176848729453\n",
      "Batch 101\n",
      "train loss:  2.580451726913452\n",
      "test loss:  3.24882333278656\n",
      "Batch 102\n",
      "train loss:  2.514404773712158\n",
      "test loss:  3.2513046662012735\n",
      "Batch 103\n",
      "train loss:  2.6162238121032715\n",
      "test loss:  3.2529478073120117\n",
      "Batch 104\n",
      "train loss:  2.7452101707458496\n",
      "test loss:  3.245048427581787\n",
      "Batch 105\n",
      "train loss:  2.490791082382202\n",
      "test loss:  3.2418330589930218\n",
      "Batch 106\n",
      "train loss:  2.644443988800049\n",
      "test loss:  3.2428107023239137\n",
      "Batch 107\n",
      "train loss:  2.641385555267334\n",
      "test loss:  3.244888257980347\n",
      "Batch 108\n",
      "train loss:  2.516649007797241\n",
      "test loss:  3.247742756207784\n",
      "Batch 109\n",
      "train loss:  2.6529855728149414\n",
      "test loss:  3.2475332736968996\n",
      "Batch 110\n",
      "train loss:  2.395941734313965\n",
      "test loss:  3.246828254063924\n",
      "Batch 111\n",
      "train loss:  2.612794876098633\n",
      "test loss:  3.2494351704915365\n",
      "Batch 112\n",
      "train loss:  2.574831962585449\n",
      "test loss:  3.2545048793156943\n",
      "Batch 113\n",
      "train loss:  2.7229740619659424\n",
      "test loss:  3.256779948870341\n",
      "Batch 114\n",
      "train loss:  2.6672120094299316\n",
      "test loss:  3.2574207544326783\n",
      "Batch 115\n",
      "train loss:  2.5502967834472656\n",
      "test loss:  3.255772002538045\n",
      "Batch 116\n",
      "train loss:  2.520843505859375\n",
      "test loss:  3.2540347814559936\n",
      "Batch 117\n",
      "train loss:  2.7168402671813965\n",
      "test loss:  3.2501025915145876\n",
      "Batch 118\n",
      "train loss:  2.6418094635009766\n",
      "test loss:  3.243384273846944\n",
      "Batch 119\n",
      "train loss:  2.5818445682525635\n",
      "test loss:  3.2377153476079306\n",
      "Batch 120\n",
      "train loss:  2.6817164421081543\n",
      "test loss:  3.2355713129043577\n",
      "Batch 121\n",
      "train loss:  2.4908227920532227\n",
      "test loss:  3.2383762041727704\n",
      "Batch 122\n",
      "train loss:  2.795687198638916\n",
      "test loss:  3.2397676944732665\n",
      "Batch 123\n",
      "train loss:  2.537748336791992\n",
      "test loss:  3.237383659680684\n",
      "Batch 124\n",
      "train loss:  2.840395212173462\n",
      "test loss:  3.2357723156611127\n",
      "Batch 125\n",
      "train loss:  2.412306785583496\n",
      "test loss:  3.2364648977915444\n",
      "Batch 126\n",
      "train loss:  2.6890647411346436\n",
      "test loss:  3.238994574546814\n",
      "Batch 127\n",
      "train loss:  2.727440357208252\n",
      "test loss:  3.2445001363754273\n",
      "Batch 128\n",
      "train loss:  2.5999300479888916\n",
      "test loss:  3.247350223859151\n",
      "Batch 129\n",
      "train loss:  2.4731006622314453\n",
      "test loss:  3.2428080638249717\n",
      "Batch 130\n",
      "train loss:  2.633528709411621\n",
      "test loss:  3.2412656784057616\n",
      "Batch 131\n",
      "train loss:  2.6831376552581787\n",
      "test loss:  3.2372551997502645\n",
      "Batch 132\n",
      "train loss:  2.781548023223877\n",
      "test loss:  3.23703769048055\n",
      "Batch 133\n",
      "train loss:  2.5549068450927734\n",
      "test loss:  3.2404802878697714\n",
      "Batch 134\n",
      "train loss:  2.625739336013794\n",
      "test loss:  3.2387059926986694\n",
      "Batch 135\n",
      "train loss:  2.698974370956421\n",
      "test loss:  3.233920955657959\n",
      "Batch 136\n",
      "train loss:  2.635262966156006\n",
      "test loss:  3.2320571899414063\n",
      "Batch 137\n",
      "train loss:  2.6642980575561523\n",
      "test loss:  3.2292833725611367\n",
      "Batch 138\n",
      "train loss:  2.660163402557373\n",
      "test loss:  3.2338235139846803\n",
      "Batch 139\n",
      "train loss:  2.5256104469299316\n",
      "test loss:  3.2383233547210692\n",
      "Batch 140\n",
      "train loss:  2.662869930267334\n",
      "test loss:  3.2393807490666706\n",
      "Batch 141\n",
      "train loss:  2.6553335189819336\n",
      "test loss:  3.2412284135818483\n",
      "Batch 142\n",
      "train loss:  2.5237722396850586\n",
      "test loss:  3.2487892468770343\n",
      "Batch 143\n",
      "train loss:  2.5575437545776367\n",
      "test loss:  3.25795160929362\n",
      "Batch 144\n",
      "train loss:  2.720986843109131\n",
      "test loss:  3.2523268699645995\n",
      "Batch 145\n",
      "train loss:  2.6714587211608887\n",
      "test loss:  3.240594665209452\n",
      "Batch 146\n",
      "train loss:  2.623976230621338\n",
      "test loss:  3.230962061882019\n",
      "Batch 147\n",
      "train loss:  2.6717801094055176\n",
      "test loss:  3.232060130437215\n",
      "Batch 148\n",
      "train loss:  2.555952787399292\n",
      "test loss:  3.238541253407796\n",
      "Batch 149\n",
      "train loss:  2.4357941150665283\n",
      "test loss:  3.245113658905029\n",
      "Batch 150\n",
      "train loss:  2.6661877632141113\n",
      "test loss:  3.2431367079416913\n",
      "Batch 151\n",
      "train loss:  2.6259446144104004\n",
      "test loss:  3.235915994644165\n",
      "Batch 152\n",
      "train loss:  2.5789549350738525\n",
      "test loss:  3.230255405108134\n",
      "Batch 153\n",
      "train loss:  2.7129156589508057\n",
      "test loss:  3.229250184694926\n",
      "Batch 154\n",
      "train loss:  2.4692163467407227\n",
      "test loss:  3.233459782600403\n",
      "Batch 155\n",
      "train loss:  2.7409634590148926\n",
      "test loss:  3.2382464011510215\n",
      "Batch 156\n",
      "train loss:  2.568469762802124\n",
      "test loss:  3.2394825140635173\n",
      "Batch 157\n",
      "train loss:  2.5830488204956055\n",
      "test loss:  3.2403135776519774\n",
      "Batch 158\n",
      "train loss:  2.785881996154785\n",
      "test loss:  3.2354294935862224\n",
      "Batch 159\n",
      "train loss:  2.5644712448120117\n",
      "test loss:  3.22695628007253\n",
      "Batch 160\n",
      "train loss:  2.673818826675415\n",
      "test loss:  3.220255406697591\n",
      "Batch 161\n",
      "train loss:  2.6397945880889893\n",
      "test loss:  3.2184109608332316\n",
      "Batch 162\n",
      "train loss:  2.58610463142395\n",
      "test loss:  3.2223796685536703\n",
      "Batch 163\n",
      "train loss:  2.823375701904297\n",
      "test loss:  3.2225340763727823\n",
      "Batch 164\n",
      "train loss:  2.614821672439575\n",
      "test loss:  3.2222689469655355\n",
      "Batch 165\n",
      "train loss:  2.5872693061828613\n",
      "test loss:  3.227703332901001\n",
      "Batch 166\n",
      "train loss:  2.759951114654541\n",
      "test loss:  3.234790285428365\n",
      "Batch 167\n",
      "train loss:  2.664414405822754\n",
      "test loss:  3.2355229536692303\n",
      "Batch 168\n",
      "train loss:  2.6190662384033203\n",
      "test loss:  3.236166795094808\n",
      "Batch 169\n",
      "train loss:  2.7282509803771973\n",
      "test loss:  3.2403014580408733\n",
      "Batch 170\n",
      "train loss:  2.675562858581543\n",
      "test loss:  3.237216107050578\n",
      "Batch 171\n",
      "train loss:  2.65684175491333\n",
      "test loss:  3.2286393562952678\n",
      "Batch 172\n",
      "train loss:  2.545073986053467\n",
      "test loss:  3.219719378153483\n",
      "Batch 173\n",
      "train loss:  2.538106918334961\n",
      "test loss:  3.2205029169718427\n",
      "Batch 174\n",
      "train loss:  2.5686702728271484\n",
      "test loss:  3.2231199502944947\n",
      "Batch 175\n",
      "train loss:  2.836599349975586\n",
      "test loss:  3.2192906697591144\n",
      "Batch 176\n",
      "train loss:  2.6099445819854736\n",
      "test loss:  3.221558753649394\n",
      "Batch 177\n",
      "train loss:  2.700028657913208\n",
      "test loss:  3.2318465232849123\n",
      "Batch 178\n",
      "train loss:  2.7270634174346924\n",
      "test loss:  3.23995881875356\n",
      "Batch 179\n",
      "train loss:  2.6520020961761475\n",
      "test loss:  3.2395297288894653\n",
      "Batch 180\n",
      "train loss:  2.6366477012634277\n",
      "test loss:  3.2328698078791303\n",
      "Batch 181\n",
      "train loss:  2.584510564804077\n",
      "test loss:  3.230703274408976\n",
      "Batch 182\n",
      "train loss:  2.764392852783203\n",
      "test loss:  3.232354998588562\n",
      "Batch 183\n",
      "train loss:  2.7064876556396484\n",
      "test loss:  3.2294261932373045\n",
      "Batch 184\n",
      "train loss:  2.661470890045166\n",
      "test loss:  3.225865395863851\n",
      "Batch 185\n",
      "train loss:  2.802835464477539\n",
      "test loss:  3.223727019627889\n",
      "Batch 186\n",
      "train loss:  2.6417622566223145\n",
      "test loss:  3.228110432624817\n",
      "Batch 187\n",
      "train loss:  2.537179708480835\n",
      "test loss:  3.231945300102234\n",
      "Batch 188\n",
      "train loss:  2.548224449157715\n",
      "test loss:  3.2348212003707886\n",
      "Batch 189\n",
      "train loss:  2.646402359008789\n",
      "test loss:  3.239046001434326\n",
      "Batch 190\n",
      "train loss:  2.4304535388946533\n",
      "test loss:  3.245757548014323\n",
      "Batch 191\n",
      "train loss:  2.816504955291748\n",
      "test loss:  3.234409562746684\n",
      "Batch 192\n",
      "train loss:  2.7536520957946777\n",
      "test loss:  3.224233055114746\n",
      "Batch 193\n",
      "train loss:  2.6294851303100586\n",
      "test loss:  3.2204405466715493\n",
      "Batch 194\n",
      "train loss:  2.5508596897125244\n",
      "test loss:  3.220667107899984\n",
      "Batch 195\n",
      "train loss:  2.8427987098693848\n",
      "test loss:  3.2242780049641926\n",
      "Batch 196\n",
      "train loss:  2.5922608375549316\n",
      "test loss:  3.2340137004852294\n",
      "Batch 197\n",
      "train loss:  2.7265706062316895\n",
      "test loss:  3.233886973063151\n",
      "Batch 198\n",
      "train loss:  2.6416773796081543\n",
      "test loss:  3.227757223447164\n",
      "Batch 199\n",
      "train loss:  2.6701910495758057\n",
      "test loss:  3.2246892849604287\n",
      "Batch 200\n",
      "train loss:  2.6948604583740234\n",
      "test loss:  3.226104211807251\n",
      "Batch 201\n",
      "train loss:  2.5782220363616943\n",
      "test loss:  3.2285319805145263\n",
      "Batch 202\n",
      "train loss:  2.7412919998168945\n",
      "test loss:  3.231638272603353\n",
      "Batch 203\n",
      "train loss:  2.638731002807617\n",
      "test loss:  3.2391575972239175\n",
      "Batch 204\n",
      "train loss:  2.598822593688965\n",
      "test loss:  3.2373069763183593\n",
      "Batch 205\n",
      "train loss:  2.7990965843200684\n",
      "test loss:  3.229354246457418\n",
      "Batch 206\n",
      "train loss:  2.565957546234131\n",
      "test loss:  3.2276842753092447\n",
      "Batch 207\n",
      "train loss:  2.5599193572998047\n",
      "test loss:  3.2327030181884764\n",
      "Batch 208\n",
      "train loss:  2.5693399906158447\n",
      "test loss:  3.233569582303365\n",
      "Batch 209\n",
      "train loss:  2.6386990547180176\n",
      "test loss:  3.2311541636784873\n",
      "Batch 210\n",
      "train loss:  2.5948853492736816\n",
      "test loss:  3.2337652444839478\n",
      "Batch 211\n",
      "train loss:  2.793980598449707\n",
      "test loss:  3.233798623085022\n",
      "Batch 212\n",
      "train loss:  2.755967855453491\n",
      "test loss:  3.2279138724009195\n",
      "Batch 213\n",
      "train loss:  2.580409526824951\n",
      "test loss:  3.2255370219548545\n",
      "Batch 214\n",
      "train loss:  2.5947647094726562\n",
      "test loss:  3.2280814170837404\n",
      "Batch 215\n",
      "train loss:  2.6515824794769287\n",
      "test loss:  3.2300652265548706\n",
      "Batch 216\n",
      "train loss:  2.7413830757141113\n",
      "test loss:  3.2328123966852824\n",
      "Batch 217\n",
      "train loss:  2.5877132415771484\n",
      "test loss:  3.2365994373957316\n",
      "Batch 218\n",
      "train loss:  2.605241298675537\n",
      "test loss:  3.2297626892725626\n",
      "Batch 219\n",
      "train loss:  2.7103490829467773\n",
      "test loss:  3.2235152959823608\n",
      "Batch 220\n",
      "train loss:  2.737550735473633\n",
      "test loss:  3.2206563154856362\n",
      "Batch 221\n",
      "train loss:  2.778916835784912\n",
      "test loss:  3.224221618970235\n",
      "Batch 222\n",
      "train loss:  2.7044806480407715\n",
      "test loss:  3.2293158054351805\n",
      "Batch 223\n",
      "train loss:  2.5477945804595947\n",
      "test loss:  3.2324944496154786\n",
      "Batch 224\n",
      "train loss:  2.598823070526123\n",
      "test loss:  3.2298373778661094\n",
      "Batch 225\n",
      "train loss:  2.6706128120422363\n",
      "test loss:  3.223605672518412\n",
      "Batch 226\n",
      "train loss:  2.708559513092041\n",
      "test loss:  3.2219226121902467\n",
      "Batch 227\n",
      "train loss:  2.743478298187256\n",
      "test loss:  3.2296282847722373\n",
      "Batch 228\n",
      "train loss:  2.573293924331665\n",
      "test loss:  3.238056683540344\n",
      "Batch 229\n",
      "train loss:  2.668907642364502\n",
      "test loss:  3.2358216126759847\n",
      "Batch 230\n",
      "train loss:  2.7342636585235596\n",
      "test loss:  3.225973947842916\n",
      "Batch 231\n",
      "train loss:  2.6231155395507812\n",
      "test loss:  3.2162007172902425\n",
      "Batch 232\n",
      "train loss:  2.623790979385376\n",
      "test loss:  3.2139029105504355\n",
      "Batch 233\n",
      "train loss:  2.7708144187927246\n",
      "test loss:  3.2100143432617188\n",
      "Batch 234\n",
      "train loss:  2.7315542697906494\n",
      "test loss:  3.208918889363607\n",
      "Batch 235\n",
      "train loss:  2.7556076049804688\n",
      "test loss:  3.2111504395802815\n",
      "Batch 236\n",
      "train loss:  2.665618896484375\n",
      "test loss:  3.2131893555323283\n",
      "Batch 237\n",
      "train loss:  2.5690155029296875\n",
      "test loss:  3.2169011433919272\n",
      "Batch 238\n",
      "train loss:  2.4700498580932617\n",
      "test loss:  3.2160125652949016\n",
      "Batch 239\n",
      "train loss:  2.7930612564086914\n",
      "test loss:  3.2125790198644\n",
      "Batch 240\n",
      "train loss:  2.6977999210357666\n",
      "test loss:  3.2128880262374877\n",
      "Batch 241\n",
      "train loss:  2.6986072063446045\n",
      "test loss:  3.215590500831604\n",
      "Batch 242\n",
      "train loss:  2.697789192199707\n",
      "test loss:  3.219729288419088\n",
      "Batch 243\n",
      "train loss:  2.6277222633361816\n",
      "test loss:  3.221333972613017\n",
      "Batch 244\n",
      "train loss:  2.7046830654144287\n",
      "test loss:  3.2160049756368\n",
      "Batch 245\n",
      "train loss:  2.74810791015625\n",
      "test loss:  3.207814033826192\n",
      "Batch 246\n",
      "train loss:  2.6414222717285156\n",
      "test loss:  3.202130095163981\n",
      "Batch 247\n",
      "train loss:  2.9621691703796387\n",
      "test loss:  3.2037611484527586\n",
      "Batch 248\n",
      "train loss:  2.680194854736328\n",
      "test loss:  3.209472664197286\n",
      "Batch 249\n",
      "train loss:  2.541595458984375\n",
      "test loss:  3.217169713973999\n",
      "Batch 250\n",
      "train loss:  2.6813807487487793\n",
      "test loss:  3.222674783070882\n",
      "Batch 251\n",
      "train loss:  2.839944362640381\n",
      "test loss:  3.2199305057525636\n",
      "Batch 252\n",
      "train loss:  2.7078516483306885\n",
      "test loss:  3.217346024513245\n",
      "Batch 253\n",
      "train loss:  2.88972806930542\n",
      "test loss:  3.2127800146738688\n",
      "Batch 254\n",
      "train loss:  2.6967101097106934\n",
      "test loss:  3.2075523773829144\n",
      "Batch 255\n",
      "train loss:  2.729109287261963\n",
      "test loss:  3.2098275820414224\n",
      "Batch 256\n",
      "train loss:  2.6447973251342773\n",
      "test loss:  3.2108843326568604\n",
      "Batch 257\n",
      "train loss:  2.6336302757263184\n",
      "test loss:  3.2068961461385093\n",
      "Batch 258\n",
      "train loss:  2.782104730606079\n",
      "test loss:  3.2026524782180785\n",
      "Batch 259\n",
      "train loss:  2.743610143661499\n",
      "test loss:  3.2080899715423583\n",
      "Batch 260\n",
      "train loss:  2.7170960903167725\n",
      "test loss:  3.2127837181091308\n",
      "Batch 261\n",
      "train loss:  2.885960578918457\n",
      "test loss:  3.212385145823161\n",
      "Batch 262\n",
      "train loss:  2.7115373611450195\n",
      "test loss:  3.2124375899632773\n",
      "output tokens:  117\n",
      "generated text:  林黛玉抓住宝玉的手说道：“你要死了，你不怕你们说我这话，你也不敢去，我也不敢去的。”宝玉道：“你们这话我也不怕你，我也不敢去了。”袭人道：“我也不敢说，你们也不敢说。”宝玉道：“你们也不敢说，    你们也该去\n",
      "Epoch 4\n",
      "Batch 1\n",
      "train loss:  2.2944071292877197\n",
      "test loss:  3.2212971289952597\n",
      "Batch 2\n",
      "train loss:  2.413472890853882\n",
      "test loss:  3.2238089005152384\n",
      "Batch 3\n",
      "train loss:  2.2564268112182617\n",
      "test loss:  3.225769543647766\n",
      "Batch 4\n",
      "train loss:  2.25907301902771\n",
      "test loss:  3.226433213551839\n",
      "Batch 5\n",
      "train loss:  2.2102019786834717\n",
      "test loss:  3.232270375887553\n",
      "Batch 6\n",
      "train loss:  2.269566059112549\n",
      "test loss:  3.238495437304179\n",
      "Batch 7\n",
      "train loss:  2.369197368621826\n",
      "test loss:  3.2380163590113322\n",
      "Batch 8\n",
      "train loss:  2.2665445804595947\n",
      "test loss:  3.2374093691507975\n",
      "Batch 9\n",
      "train loss:  2.189972400665283\n",
      "test loss:  3.2395122130711873\n",
      "Batch 10\n",
      "train loss:  2.2467355728149414\n",
      "test loss:  3.2455686966578168\n",
      "Batch 11\n",
      "train loss:  2.295456647872925\n",
      "test loss:  3.241202624638875\n",
      "Batch 12\n",
      "train loss:  2.3382318019866943\n",
      "test loss:  3.2343831698099774\n",
      "Batch 13\n",
      "train loss:  2.307446241378784\n",
      "test loss:  3.2354493538538613\n",
      "Batch 14\n",
      "train loss:  2.3073744773864746\n",
      "test loss:  3.239420946439107\n",
      "Batch 15\n",
      "train loss:  2.1676156520843506\n",
      "test loss:  3.2455432335535686\n",
      "Batch 16\n",
      "train loss:  2.346205234527588\n",
      "test loss:  3.2548669974009194\n",
      "Batch 17\n",
      "train loss:  2.1497583389282227\n",
      "test loss:  3.256095862388611\n",
      "Batch 18\n",
      "train loss:  2.1804089546203613\n",
      "test loss:  3.255619589487712\n",
      "Batch 19\n",
      "train loss:  2.3024792671203613\n",
      "test loss:  3.254882272084554\n",
      "Batch 20\n",
      "train loss:  2.2895772457122803\n",
      "test loss:  3.2538781642913817\n",
      "Batch 21\n",
      "train loss:  2.2095112800598145\n",
      "test loss:  3.256404662132263\n",
      "Batch 22\n",
      "train loss:  2.356837749481201\n",
      "test loss:  3.2643480698267617\n",
      "Batch 23\n",
      "train loss:  2.2963571548461914\n",
      "test loss:  3.268366058667501\n",
      "Batch 24\n",
      "train loss:  2.183971405029297\n",
      "test loss:  3.2707326809565225\n",
      "Batch 25\n",
      "train loss:  2.1826162338256836\n",
      "test loss:  3.2665145635604858\n",
      "Batch 26\n",
      "train loss:  2.170430898666382\n",
      "test loss:  3.264504861831665\n",
      "Batch 27\n",
      "train loss:  2.303959846496582\n",
      "test loss:  3.265986204147339\n",
      "Batch 28\n",
      "train loss:  2.194037914276123\n",
      "test loss:  3.2661095380783083\n",
      "Batch 29\n",
      "train loss:  2.3381097316741943\n",
      "test loss:  3.268656547864278\n",
      "Batch 30\n",
      "train loss:  2.378970146179199\n",
      "test loss:  3.2679415225982664\n",
      "Batch 31\n",
      "train loss:  2.1221189498901367\n",
      "test loss:  3.270075273513794\n",
      "Batch 32\n",
      "train loss:  2.295070171356201\n",
      "test loss:  3.2700852394104003\n",
      "Batch 33\n",
      "train loss:  2.256713390350342\n",
      "test loss:  3.273054019610087\n",
      "Batch 34\n",
      "train loss:  2.229823112487793\n",
      "test loss:  3.27938973903656\n",
      "Batch 35\n",
      "train loss:  2.3269810676574707\n",
      "test loss:  3.2801233371098837\n",
      "Batch 36\n",
      "train loss:  2.3182196617126465\n",
      "test loss:  3.281050435702006\n",
      "Batch 37\n",
      "train loss:  2.173675775527954\n",
      "test loss:  3.2887900829315186\n",
      "Batch 38\n",
      "train loss:  2.1360974311828613\n",
      "test loss:  3.2902003129323325\n",
      "Batch 39\n",
      "train loss:  2.1986639499664307\n",
      "test loss:  3.285982131958008\n",
      "Batch 40\n",
      "train loss:  2.223830461502075\n",
      "test loss:  3.2794368187586467\n",
      "Batch 41\n",
      "train loss:  2.208017349243164\n",
      "test loss:  3.27919180393219\n",
      "Batch 42\n",
      "train loss:  2.1406357288360596\n",
      "test loss:  3.2841686328252155\n",
      "Batch 43\n",
      "train loss:  2.2179794311523438\n",
      "test loss:  3.29142853418986\n",
      "Batch 44\n",
      "train loss:  2.408752918243408\n",
      "test loss:  3.29907128016154\n",
      "Batch 45\n",
      "train loss:  2.2114694118499756\n",
      "test loss:  3.3107095400492352\n",
      "Batch 46\n",
      "train loss:  2.356581449508667\n",
      "test loss:  3.3060609340667724\n",
      "Batch 47\n",
      "train loss:  2.253070831298828\n",
      "test loss:  3.301204490661621\n",
      "Batch 48\n",
      "train loss:  2.4272100925445557\n",
      "test loss:  3.298004674911499\n",
      "Batch 49\n",
      "train loss:  2.132502794265747\n",
      "test loss:  3.296109954516093\n",
      "Batch 50\n",
      "train loss:  2.3160126209259033\n",
      "test loss:  3.296181400616964\n",
      "Batch 51\n",
      "train loss:  2.2616405487060547\n",
      "test loss:  3.3003944396972655\n",
      "Batch 52\n",
      "train loss:  2.1770262718200684\n",
      "test loss:  3.301624568303426\n",
      "Batch 53\n",
      "train loss:  2.214991569519043\n",
      "test loss:  3.302532267570496\n",
      "Batch 54\n",
      "train loss:  2.202852249145508\n",
      "test loss:  3.3107411940892537\n",
      "Batch 55\n",
      "train loss:  2.1865835189819336\n",
      "test loss:  3.313662870724996\n",
      "Batch 56\n",
      "train loss:  2.2625911235809326\n",
      "test loss:  3.30811558564504\n",
      "Batch 57\n",
      "train loss:  2.397855281829834\n",
      "test loss:  3.3046321630477906\n",
      "Batch 58\n",
      "train loss:  2.2405307292938232\n",
      "test loss:  3.302890086174011\n",
      "Batch 59\n",
      "train loss:  2.1809864044189453\n",
      "test loss:  3.301273234685262\n",
      "Batch 60\n",
      "train loss:  2.169905185699463\n",
      "test loss:  3.3002610921859743\n",
      "Batch 61\n",
      "train loss:  2.1622471809387207\n",
      "test loss:  3.307989796002706\n",
      "Batch 62\n",
      "train loss:  2.2509684562683105\n",
      "test loss:  3.3105430682500203\n",
      "Batch 63\n",
      "train loss:  2.355496406555176\n",
      "test loss:  3.309287357330322\n",
      "Batch 64\n",
      "train loss:  2.2079384326934814\n",
      "test loss:  3.3115413506825764\n",
      "Batch 65\n",
      "train loss:  2.206803321838379\n",
      "test loss:  3.3155147155125935\n",
      "Batch 66\n",
      "train loss:  2.206807851791382\n",
      "test loss:  3.3078842719395953\n",
      "Batch 67\n",
      "train loss:  2.236815929412842\n",
      "test loss:  3.304229211807251\n",
      "Batch 68\n",
      "train loss:  2.3005144596099854\n",
      "test loss:  3.296786665916443\n",
      "Batch 69\n",
      "train loss:  2.169048547744751\n",
      "test loss:  3.2958532333374024\n",
      "Batch 70\n",
      "train loss:  2.3386454582214355\n",
      "test loss:  3.3019976298014324\n",
      "Batch 71\n",
      "train loss:  2.326315402984619\n",
      "test loss:  3.3128315925598146\n",
      "Batch 72\n",
      "train loss:  2.2951927185058594\n",
      "test loss:  3.318322706222534\n",
      "Batch 73\n",
      "train loss:  2.454483985900879\n",
      "test loss:  3.3107590834299723\n",
      "Batch 74\n",
      "train loss:  2.2916629314422607\n",
      "test loss:  3.313158655166626\n",
      "Batch 75\n",
      "train loss:  2.3086681365966797\n",
      "test loss:  3.3175002098083497\n",
      "Batch 76\n",
      "train loss:  2.223083019256592\n",
      "test loss:  3.317104959487915\n",
      "Batch 77\n",
      "train loss:  2.225017547607422\n",
      "test loss:  3.319404673576355\n",
      "Batch 78\n",
      "train loss:  2.1278088092803955\n",
      "test loss:  3.3204745054244995\n",
      "Batch 79\n",
      "train loss:  2.2622199058532715\n",
      "test loss:  3.3205417950948077\n",
      "Batch 80\n",
      "train loss:  2.3547797203063965\n",
      "test loss:  3.317730744679769\n",
      "Batch 81\n",
      "train loss:  2.405043601989746\n",
      "test loss:  3.31594553788503\n",
      "Batch 82\n",
      "train loss:  2.09067440032959\n",
      "test loss:  3.3117315848668416\n",
      "Batch 83\n",
      "train loss:  2.45485782623291\n",
      "test loss:  3.310016131401062\n",
      "Batch 84\n",
      "train loss:  2.2701778411865234\n",
      "test loss:  3.3133599122365314\n",
      "Batch 85\n",
      "train loss:  2.234893798828125\n",
      "test loss:  3.313830820719401\n",
      "Batch 86\n",
      "train loss:  2.1803157329559326\n",
      "test loss:  3.3146378993988037\n",
      "Batch 87\n",
      "train loss:  2.2198190689086914\n",
      "test loss:  3.3177480856577555\n",
      "Batch 88\n",
      "train loss:  2.2526984214782715\n",
      "test loss:  3.3219931920369468\n",
      "Batch 89\n",
      "train loss:  2.3171744346618652\n",
      "test loss:  3.324289600054423\n",
      "Batch 90\n",
      "train loss:  2.383078098297119\n",
      "test loss:  3.325695816675822\n",
      "Batch 91\n",
      "train loss:  2.2449862957000732\n",
      "test loss:  3.328947416941325\n",
      "Batch 92\n",
      "train loss:  2.2441422939300537\n",
      "test loss:  3.3284474929173786\n",
      "Batch 93\n",
      "train loss:  2.4162535667419434\n",
      "test loss:  3.3270440260569254\n",
      "Batch 94\n",
      "train loss:  2.4248013496398926\n",
      "test loss:  3.3252081553141277\n",
      "Batch 95\n",
      "train loss:  2.3654680252075195\n",
      "test loss:  3.3273840586344403\n",
      "Batch 96\n",
      "train loss:  2.2477715015411377\n",
      "test loss:  3.330728809038798\n",
      "Batch 97\n",
      "train loss:  2.3777928352355957\n",
      "test loss:  3.329879132906596\n",
      "Batch 98\n",
      "train loss:  2.278226375579834\n",
      "test loss:  3.3274187485376996\n",
      "Batch 99\n",
      "train loss:  2.2537198066711426\n",
      "test loss:  3.328637782732646\n",
      "Batch 100\n",
      "train loss:  2.2065579891204834\n",
      "test loss:  3.3294147729873655\n",
      "Batch 101\n",
      "train loss:  2.2695155143737793\n",
      "test loss:  3.3263869365056355\n",
      "Batch 102\n",
      "train loss:  2.4751009941101074\n",
      "test loss:  3.3238484064737954\n",
      "Batch 103\n",
      "train loss:  2.190662384033203\n",
      "test loss:  3.325043296813965\n",
      "Batch 104\n",
      "train loss:  2.213502883911133\n",
      "test loss:  3.3224475304285686\n",
      "Batch 105\n",
      "train loss:  2.408602714538574\n",
      "test loss:  3.322451122601827\n",
      "Batch 106\n",
      "train loss:  2.2998147010803223\n",
      "test loss:  3.3234919706980386\n",
      "Batch 107\n",
      "train loss:  2.239042043685913\n",
      "test loss:  3.324331204096476\n",
      "Batch 108\n",
      "train loss:  2.3947079181671143\n",
      "test loss:  3.3216938813527426\n",
      "Batch 109\n",
      "train loss:  2.181626319885254\n",
      "test loss:  3.317400010426839\n",
      "Batch 110\n",
      "train loss:  2.2257237434387207\n",
      "test loss:  3.319481444358826\n",
      "Batch 111\n",
      "train loss:  2.259739637374878\n",
      "test loss:  3.3285823106765746\n",
      "Batch 112\n",
      "train loss:  2.1864709854125977\n",
      "test loss:  3.32979408899943\n",
      "Batch 113\n",
      "train loss:  2.2584891319274902\n",
      "test loss:  3.3260718107223513\n",
      "Batch 114\n",
      "train loss:  2.3240966796875\n",
      "test loss:  3.3219388008117674\n",
      "Batch 115\n",
      "train loss:  2.183809757232666\n",
      "test loss:  3.322136727968852\n",
      "Batch 116\n",
      "train loss:  2.2612509727478027\n",
      "test loss:  3.3207711378733316\n",
      "Batch 117\n",
      "train loss:  2.188412666320801\n",
      "test loss:  3.325369048118591\n",
      "Batch 118\n",
      "train loss:  2.421739339828491\n",
      "test loss:  3.325535011291504\n",
      "Batch 119\n",
      "train loss:  2.2254140377044678\n",
      "test loss:  3.32451446056366\n",
      "Batch 120\n",
      "train loss:  2.383889675140381\n",
      "test loss:  3.3267330646514894\n",
      "Batch 121\n",
      "train loss:  2.337415933609009\n",
      "test loss:  3.3298842589060467\n",
      "Batch 122\n",
      "train loss:  2.2989978790283203\n",
      "test loss:  3.3285327593485516\n",
      "Batch 123\n",
      "train loss:  2.318209409713745\n",
      "test loss:  3.326479371388753\n",
      "Batch 124\n",
      "train loss:  2.3059237003326416\n",
      "test loss:  3.322239947319031\n",
      "Batch 125\n",
      "train loss:  2.3462555408477783\n",
      "test loss:  3.317668835322062\n",
      "Batch 126\n",
      "train loss:  2.3167800903320312\n",
      "test loss:  3.32140949567159\n",
      "Batch 127\n",
      "train loss:  2.416527271270752\n",
      "test loss:  3.328217093149821\n",
      "Batch 128\n",
      "train loss:  2.4422383308410645\n",
      "test loss:  3.325054430961609\n",
      "Batch 129\n",
      "train loss:  2.481659412384033\n",
      "test loss:  3.319031604131063\n",
      "Batch 130\n",
      "train loss:  2.2778749465942383\n",
      "test loss:  3.3119113365809123\n",
      "Batch 131\n",
      "train loss:  2.3483216762542725\n",
      "test loss:  3.305757482846578\n",
      "Batch 132\n",
      "train loss:  2.425025701522827\n",
      "test loss:  3.3046529134114584\n",
      "Batch 133\n",
      "train loss:  2.463776111602783\n",
      "test loss:  3.3093124628067017\n",
      "Batch 134\n",
      "train loss:  2.4236037731170654\n",
      "test loss:  3.3201512813568117\n",
      "Batch 135\n",
      "train loss:  2.395954132080078\n",
      "test loss:  3.332242250442505\n",
      "Batch 136\n",
      "train loss:  2.3914918899536133\n",
      "test loss:  3.33723931312561\n",
      "Batch 137\n",
      "train loss:  2.161781072616577\n",
      "test loss:  3.3321406841278076\n",
      "Batch 138\n",
      "train loss:  2.35021710395813\n",
      "test loss:  3.3268444617589314\n",
      "Batch 139\n",
      "train loss:  2.4345717430114746\n",
      "test loss:  3.323178275426229\n",
      "Batch 140\n",
      "train loss:  2.2985141277313232\n",
      "test loss:  3.324644907315572\n",
      "Batch 141\n",
      "train loss:  2.338176727294922\n",
      "test loss:  3.3194103558858234\n",
      "Batch 142\n",
      "train loss:  2.4435882568359375\n",
      "test loss:  3.312412802378337\n",
      "Batch 143\n",
      "train loss:  2.4383959770202637\n",
      "test loss:  3.3096144835154218\n",
      "Batch 144\n",
      "train loss:  2.3834171295166016\n",
      "test loss:  3.3123919566472373\n",
      "Batch 145\n",
      "train loss:  2.3546018600463867\n",
      "test loss:  3.317434485753377\n",
      "Batch 146\n",
      "train loss:  2.342641830444336\n",
      "test loss:  3.3260218540827435\n",
      "Batch 147\n",
      "train loss:  2.2594246864318848\n",
      "test loss:  3.320855696996053\n",
      "Batch 148\n",
      "train loss:  2.278336524963379\n",
      "test loss:  3.3127310037612916\n",
      "Batch 149\n",
      "train loss:  2.299738645553589\n",
      "test loss:  3.3115596612294516\n",
      "Batch 150\n",
      "train loss:  2.314148187637329\n",
      "test loss:  3.314031942685445\n",
      "Batch 151\n",
      "train loss:  2.4419989585876465\n",
      "test loss:  3.3184067249298095\n",
      "Batch 152\n",
      "train loss:  2.168036937713623\n",
      "test loss:  3.326505454381307\n",
      "Batch 153\n",
      "train loss:  2.3809680938720703\n",
      "test loss:  3.3315700848897296\n",
      "Batch 154\n",
      "train loss:  2.2010722160339355\n",
      "test loss:  3.322346083323161\n",
      "Batch 155\n",
      "train loss:  2.4235496520996094\n",
      "test loss:  3.3130459070205687\n",
      "Batch 156\n",
      "train loss:  2.4079935550689697\n",
      "test loss:  3.3062899430592854\n",
      "Batch 157\n",
      "train loss:  2.312927722930908\n",
      "test loss:  3.3084139506022137\n",
      "Batch 158\n",
      "train loss:  2.2665610313415527\n",
      "test loss:  3.3141757885615033\n",
      "Batch 159\n",
      "train loss:  2.40959095954895\n",
      "test loss:  3.325304158528646\n",
      "Batch 160\n",
      "train loss:  2.56502366065979\n",
      "test loss:  3.326547956466675\n",
      "Batch 161\n",
      "train loss:  2.23551082611084\n",
      "test loss:  3.325294319788615\n",
      "Batch 162\n",
      "train loss:  2.2843968868255615\n",
      "test loss:  3.3192684014638263\n",
      "Batch 163\n",
      "train loss:  2.526273488998413\n",
      "test loss:  3.310760188102722\n",
      "Batch 164\n",
      "train loss:  2.451669216156006\n",
      "test loss:  3.30698192914327\n",
      "Batch 165\n",
      "train loss:  2.3240556716918945\n",
      "test loss:  3.309863630930583\n",
      "Batch 166\n",
      "train loss:  2.3825998306274414\n",
      "test loss:  3.3159379800160727\n",
      "Batch 167\n",
      "train loss:  2.3452982902526855\n",
      "test loss:  3.3252567370732624\n",
      "Batch 168\n",
      "train loss:  2.3597164154052734\n",
      "test loss:  3.330651942888896\n",
      "Batch 169\n",
      "train loss:  2.4595344066619873\n",
      "test loss:  3.322923501332601\n",
      "Batch 170\n",
      "train loss:  2.3098413944244385\n",
      "test loss:  3.3186329205830893\n",
      "Batch 171\n",
      "train loss:  2.343540668487549\n",
      "test loss:  3.319739580154419\n",
      "Batch 172\n",
      "train loss:  2.2665205001831055\n",
      "test loss:  3.3240365266799925\n",
      "Batch 173\n",
      "train loss:  2.3560895919799805\n",
      "test loss:  3.3244397401809693\n",
      "Batch 174\n",
      "train loss:  2.4009909629821777\n",
      "test loss:  3.323005533218384\n",
      "Batch 175\n",
      "train loss:  2.4455766677856445\n",
      "test loss:  3.3248603026072185\n",
      "Batch 176\n",
      "train loss:  2.3908591270446777\n",
      "test loss:  3.3305131514867146\n",
      "Batch 177\n",
      "train loss:  2.2974965572357178\n",
      "test loss:  3.3292259852091473\n",
      "Batch 178\n",
      "train loss:  2.311445474624634\n",
      "test loss:  3.3246256510416665\n",
      "Batch 179\n",
      "train loss:  2.4571878910064697\n",
      "test loss:  3.3211696704228717\n",
      "Batch 180\n",
      "train loss:  2.452777862548828\n",
      "test loss:  3.318530321121216\n",
      "Batch 181\n",
      "train loss:  2.439558267593384\n",
      "test loss:  3.316045880317688\n",
      "Batch 182\n",
      "train loss:  2.3423547744750977\n",
      "test loss:  3.315236775080363\n",
      "Batch 183\n",
      "train loss:  2.298790216445923\n",
      "test loss:  3.3177677631378173\n",
      "Batch 184\n",
      "train loss:  2.337930679321289\n",
      "test loss:  3.320494023958842\n",
      "Batch 185\n",
      "train loss:  2.3798584938049316\n",
      "test loss:  3.3235728263854982\n",
      "Batch 186\n",
      "train loss:  2.5288898944854736\n",
      "test loss:  3.319613989194234\n",
      "Batch 187\n",
      "train loss:  2.47829532623291\n",
      "test loss:  3.312737401326497\n",
      "Batch 188\n",
      "train loss:  2.3297526836395264\n",
      "test loss:  3.307869005203247\n",
      "Batch 189\n",
      "train loss:  2.4014573097229004\n",
      "test loss:  3.3066946029663087\n",
      "Batch 190\n",
      "train loss:  2.3189034461975098\n",
      "test loss:  3.3123500108718873\n",
      "Batch 191\n",
      "train loss:  2.3578684329986572\n",
      "test loss:  3.315013591448466\n",
      "Batch 192\n",
      "train loss:  2.488978385925293\n",
      "test loss:  3.3132641951243085\n",
      "Batch 193\n",
      "train loss:  2.381423234939575\n",
      "test loss:  3.314995511372884\n",
      "Batch 194\n",
      "train loss:  2.460693359375\n",
      "test loss:  3.312078928947449\n",
      "Batch 195\n",
      "train loss:  2.414457082748413\n",
      "test loss:  3.3083161036173503\n",
      "Batch 196\n",
      "train loss:  2.5126535892486572\n",
      "test loss:  3.309170659383138\n",
      "Batch 197\n",
      "train loss:  2.317849636077881\n",
      "test loss:  3.3101840098698934\n",
      "Batch 198\n",
      "train loss:  2.2113447189331055\n",
      "test loss:  3.310532331466675\n",
      "Batch 199\n",
      "train loss:  2.438216209411621\n",
      "test loss:  3.3089030583699546\n",
      "Batch 200\n",
      "train loss:  2.336064100265503\n",
      "test loss:  3.3083110253016152\n",
      "Batch 201\n",
      "train loss:  2.422520160675049\n",
      "test loss:  3.3093598524729413\n",
      "Batch 202\n",
      "train loss:  2.3592123985290527\n",
      "test loss:  3.3128595908482867\n",
      "Batch 203\n",
      "train loss:  2.1136884689331055\n",
      "test loss:  3.3212162256240845\n",
      "Batch 204\n",
      "train loss:  2.3308253288269043\n",
      "test loss:  3.325855843226115\n",
      "Batch 205\n",
      "train loss:  2.2133185863494873\n",
      "test loss:  3.3206464052200317\n",
      "Batch 206\n",
      "train loss:  2.3057501316070557\n",
      "test loss:  3.3141162157058717\n",
      "Batch 207\n",
      "train loss:  2.334625244140625\n",
      "test loss:  3.309726365407308\n",
      "Batch 208\n",
      "train loss:  2.3497190475463867\n",
      "test loss:  3.310964266459147\n",
      "Batch 209\n",
      "train loss:  2.4759678840637207\n",
      "test loss:  3.3120649417241412\n",
      "Batch 210\n",
      "train loss:  2.5150809288024902\n",
      "test loss:  3.3143481095631917\n",
      "Batch 211\n",
      "train loss:  2.3779373168945312\n",
      "test loss:  3.314956521987915\n",
      "Batch 212\n",
      "train loss:  2.3743669986724854\n",
      "test loss:  3.315460292498271\n",
      "Batch 213\n",
      "train loss:  2.398061752319336\n",
      "test loss:  3.3120663007100424\n",
      "Batch 214\n",
      "train loss:  2.296562433242798\n",
      "test loss:  3.3086408297220866\n",
      "Batch 215\n",
      "train loss:  2.5612711906433105\n",
      "test loss:  3.3102849324544272\n",
      "Batch 216\n",
      "train loss:  2.439819812774658\n",
      "test loss:  3.3114409923553465\n",
      "Batch 217\n",
      "train loss:  2.4271888732910156\n",
      "test loss:  3.306183131535848\n",
      "Batch 218\n",
      "train loss:  2.3357815742492676\n",
      "test loss:  3.3015892187754314\n",
      "Batch 219\n",
      "train loss:  2.546067476272583\n",
      "test loss:  3.3102333704630533\n",
      "Batch 220\n",
      "train loss:  2.3710858821868896\n",
      "test loss:  3.3164448022842405\n",
      "Batch 221\n",
      "train loss:  2.3017215728759766\n",
      "test loss:  3.3144940455754597\n",
      "Batch 222\n",
      "train loss:  2.4148871898651123\n",
      "test loss:  3.311813720067342\n",
      "Batch 223\n",
      "train loss:  2.2726736068725586\n",
      "test loss:  3.3129846811294557\n",
      "Batch 224\n",
      "train loss:  2.265108108520508\n",
      "test loss:  3.318324605623881\n",
      "Batch 225\n",
      "train loss:  2.528871774673462\n",
      "test loss:  3.3199862162272136\n",
      "Batch 226\n",
      "train loss:  2.2317237854003906\n",
      "test loss:  3.3214656352996825\n",
      "Batch 227\n",
      "train loss:  2.442796468734741\n",
      "test loss:  3.317046324412028\n",
      "Batch 228\n",
      "train loss:  2.3814780712127686\n",
      "test loss:  3.316298158963521\n",
      "Batch 229\n",
      "train loss:  2.408914089202881\n",
      "test loss:  3.318034060796102\n",
      "Batch 230\n",
      "train loss:  2.457838296890259\n",
      "test loss:  3.314763061205546\n",
      "Batch 231\n",
      "train loss:  2.381471633911133\n",
      "test loss:  3.3130330244700112\n",
      "Batch 232\n",
      "train loss:  2.4214138984680176\n",
      "test loss:  3.311633253097534\n",
      "Batch 233\n",
      "train loss:  2.2952544689178467\n",
      "test loss:  3.3142321983973186\n",
      "Batch 234\n",
      "train loss:  2.446312665939331\n",
      "test loss:  3.3142640272776287\n",
      "Batch 235\n",
      "train loss:  2.523756980895996\n",
      "test loss:  3.312667735417684\n",
      "Batch 236\n",
      "train loss:  2.4805939197540283\n",
      "test loss:  3.3134106238683065\n",
      "Batch 237\n",
      "train loss:  2.25287127494812\n",
      "test loss:  3.307746974627177\n",
      "Batch 238\n",
      "train loss:  2.346910238265991\n",
      "test loss:  3.305956808725993\n",
      "Batch 239\n",
      "train loss:  2.439821481704712\n",
      "test loss:  3.3086987177530927\n",
      "Batch 240\n",
      "train loss:  2.575511932373047\n",
      "test loss:  3.309053087234497\n",
      "Batch 241\n",
      "train loss:  2.3725616931915283\n",
      "test loss:  3.309567435582479\n",
      "Batch 242\n",
      "train loss:  2.424593925476074\n",
      "test loss:  3.3101176023483276\n",
      "Batch 243\n",
      "train loss:  2.462766408920288\n",
      "test loss:  3.309247390429179\n",
      "Batch 244\n",
      "train loss:  2.3291749954223633\n",
      "test loss:  3.308663074175517\n",
      "Batch 245\n",
      "train loss:  2.415213108062744\n",
      "test loss:  3.307354760169983\n",
      "Batch 246\n",
      "train loss:  2.374924659729004\n",
      "test loss:  3.3079742670059202\n",
      "Batch 247\n",
      "train loss:  2.3516530990600586\n",
      "test loss:  3.3151350895563763\n",
      "Batch 248\n",
      "train loss:  2.339338779449463\n",
      "test loss:  3.3148053089777627\n",
      "Batch 249\n",
      "train loss:  2.615218162536621\n",
      "test loss:  3.3059222380320232\n",
      "Batch 250\n",
      "train loss:  2.42063045501709\n",
      "test loss:  3.2984211365381877\n",
      "Batch 251\n",
      "train loss:  2.326888084411621\n",
      "test loss:  3.300994348526001\n",
      "Batch 252\n",
      "train loss:  2.4320435523986816\n",
      "test loss:  3.3119351466496787\n",
      "Batch 253\n",
      "train loss:  2.2548599243164062\n",
      "test loss:  3.318531942367554\n",
      "Batch 254\n",
      "train loss:  2.3786864280700684\n",
      "test loss:  3.315748182932536\n",
      "Batch 255\n",
      "train loss:  2.2658114433288574\n",
      "test loss:  3.310572560628255\n",
      "Batch 256\n",
      "train loss:  2.4902777671813965\n",
      "test loss:  3.303805875778198\n",
      "Batch 257\n",
      "train loss:  2.4648590087890625\n",
      "test loss:  3.3023539463678997\n",
      "Batch 258\n",
      "train loss:  2.2482876777648926\n",
      "test loss:  3.306007750829061\n",
      "Batch 259\n",
      "train loss:  2.5040335655212402\n",
      "test loss:  3.311494716008504\n",
      "Batch 260\n",
      "train loss:  2.450758934020996\n",
      "test loss:  3.314636540412903\n",
      "Batch 261\n",
      "train loss:  2.5975427627563477\n",
      "test loss:  3.315802224477132\n",
      "Batch 262\n",
      "train loss:  2.404616355895996\n",
      "test loss:  3.31655531724294\n",
      "output tokens:  117\n",
      "generated text:  林黛玉抓住宝玉的手说道：“你们家里有这个理，这会子又来叫人偷偷了去呢．    \"宝玉笑道：“你说我不说，我说你不敢。”黛玉道：“你不许你听听，我听见说话了，我就信了。”宝玉道：“你也不用说，你说我听听说的是谁，\n",
      "Epoch 5\n",
      "Batch 1\n",
      "train loss:  1.8206799030303955\n",
      "test loss:  3.326153786977132\n",
      "Batch 2\n",
      "train loss:  1.7283397912979126\n",
      "test loss:  3.337886579831441\n",
      "Batch 3\n",
      "train loss:  1.8988113403320312\n",
      "test loss:  3.346376975377401\n",
      "Batch 4\n",
      "train loss:  1.9264860153198242\n",
      "test loss:  3.35153075059255\n",
      "Batch 5\n",
      "train loss:  2.0088276863098145\n",
      "test loss:  3.354524866739909\n",
      "Batch 6\n",
      "train loss:  1.7440037727355957\n",
      "test loss:  3.3562597751617433\n",
      "Batch 7\n",
      "train loss:  1.855360507965088\n",
      "test loss:  3.3632929642995197\n",
      "Batch 8\n",
      "train loss:  1.7428102493286133\n",
      "test loss:  3.3700055996576945\n",
      "Batch 9\n",
      "train loss:  1.7975866794586182\n",
      "test loss:  3.373767638206482\n",
      "Batch 10\n",
      "train loss:  1.9796558618545532\n",
      "test loss:  3.3731053908665976\n",
      "Batch 11\n",
      "train loss:  1.9418892860412598\n",
      "test loss:  3.375672483444214\n",
      "Batch 12\n",
      "train loss:  1.8400630950927734\n",
      "test loss:  3.379065823554993\n",
      "Batch 13\n",
      "train loss:  1.838891863822937\n",
      "test loss:  3.38955078125\n",
      "Batch 14\n",
      "train loss:  1.7219514846801758\n",
      "test loss:  3.4007755120595298\n",
      "Batch 15\n",
      "train loss:  1.9334172010421753\n",
      "test loss:  3.3990856329600017\n",
      "Batch 16\n",
      "train loss:  1.9413282871246338\n",
      "test loss:  3.3976737022399903\n",
      "Batch 17\n",
      "train loss:  1.7542026042938232\n",
      "test loss:  3.4022694985071817\n",
      "Batch 18\n",
      "train loss:  1.9367899894714355\n",
      "test loss:  3.4031128247578937\n",
      "Batch 19\n",
      "train loss:  1.928945541381836\n",
      "test loss:  3.405540951093038\n",
      "Batch 20\n",
      "train loss:  1.7708598375320435\n",
      "test loss:  3.4060161034266154\n",
      "Batch 21\n",
      "train loss:  1.9590351581573486\n",
      "test loss:  3.4019752820332845\n",
      "Batch 22\n",
      "train loss:  1.7546697854995728\n",
      "test loss:  3.404397392272949\n",
      "Batch 23\n",
      "train loss:  1.8859843015670776\n",
      "test loss:  3.4099858601888022\n",
      "Batch 24\n",
      "train loss:  1.8897038698196411\n",
      "test loss:  3.4136717955271405\n",
      "Batch 25\n",
      "train loss:  1.9959925413131714\n",
      "test loss:  3.4128864924112956\n",
      "Batch 26\n",
      "train loss:  2.0922112464904785\n",
      "test loss:  3.414317989349365\n",
      "Batch 27\n",
      "train loss:  1.8932011127471924\n",
      "test loss:  3.4178133249282836\n",
      "Batch 28\n",
      "train loss:  1.7359883785247803\n",
      "test loss:  3.425779914855957\n",
      "Batch 29\n",
      "train loss:  1.9249417781829834\n",
      "test loss:  3.43039391040802\n",
      "Batch 30\n",
      "train loss:  1.867499828338623\n",
      "test loss:  3.433698026339213\n",
      "Batch 31\n",
      "train loss:  1.8082282543182373\n",
      "test loss:  3.435291035970052\n",
      "Batch 32\n",
      "train loss:  1.8938570022583008\n",
      "test loss:  3.430731185277303\n",
      "Batch 33\n",
      "train loss:  1.8982316255569458\n",
      "test loss:  3.426498516400655\n",
      "Batch 34\n",
      "train loss:  1.7729508876800537\n",
      "test loss:  3.424283822377523\n",
      "Batch 35\n",
      "train loss:  1.7615339756011963\n",
      "test loss:  3.423718881607056\n",
      "Batch 36\n",
      "train loss:  1.8775960206985474\n",
      "test loss:  3.4251981655756634\n",
      "Batch 37\n",
      "train loss:  1.8430370092391968\n",
      "test loss:  3.428269608815511\n",
      "Batch 38\n",
      "train loss:  2.007099151611328\n",
      "test loss:  3.4303112983703614\n",
      "Batch 39\n",
      "train loss:  1.8930872678756714\n",
      "test loss:  3.433449363708496\n",
      "Batch 40\n",
      "train loss:  1.7821271419525146\n",
      "test loss:  3.438124736150106\n",
      "Batch 41\n",
      "train loss:  1.810565710067749\n",
      "test loss:  3.4411238272984823\n",
      "Batch 42\n",
      "train loss:  1.9031033515930176\n",
      "test loss:  3.440605115890503\n",
      "Batch 43\n",
      "train loss:  1.8999097347259521\n",
      "test loss:  3.4393970012664794\n",
      "Batch 44\n",
      "train loss:  1.6867694854736328\n",
      "test loss:  3.445686189333598\n",
      "Batch 45\n",
      "train loss:  1.831231951713562\n",
      "test loss:  3.444117561976115\n",
      "Batch 46\n",
      "train loss:  1.971494436264038\n",
      "test loss:  3.4450151681900025\n",
      "Batch 47\n",
      "train loss:  1.8909704685211182\n",
      "test loss:  3.454797943433126\n",
      "Batch 48\n",
      "train loss:  1.847657322883606\n",
      "test loss:  3.4654455105463664\n",
      "Batch 49\n",
      "train loss:  1.914888858795166\n",
      "test loss:  3.4674636920293174\n",
      "Batch 50\n",
      "train loss:  1.9263416528701782\n",
      "test loss:  3.4611148754755656\n",
      "Batch 51\n",
      "train loss:  1.9125142097473145\n",
      "test loss:  3.451386801401774\n",
      "Batch 52\n",
      "train loss:  1.8253450393676758\n",
      "test loss:  3.451186426480611\n",
      "Batch 53\n",
      "train loss:  1.9578123092651367\n",
      "test loss:  3.4557424942652384\n",
      "Batch 54\n",
      "train loss:  1.774223804473877\n",
      "test loss:  3.458553179105123\n",
      "Batch 55\n",
      "train loss:  1.8142592906951904\n",
      "test loss:  3.4652913411458335\n",
      "Batch 56\n",
      "train loss:  1.784632921218872\n",
      "test loss:  3.472573733329773\n",
      "Batch 57\n",
      "train loss:  1.7024260759353638\n",
      "test loss:  3.4724442958831787\n",
      "Batch 58\n",
      "train loss:  1.8993104696273804\n",
      "test loss:  3.472707724571228\n",
      "Batch 59\n",
      "train loss:  1.8777148723602295\n",
      "test loss:  3.4671594142913817\n",
      "Batch 60\n",
      "train loss:  1.9488964080810547\n",
      "test loss:  3.4625438928604124\n",
      "Batch 61\n",
      "train loss:  1.8146965503692627\n",
      "test loss:  3.459491729736328\n",
      "Batch 62\n",
      "train loss:  1.8542546033859253\n",
      "test loss:  3.4600537220637\n",
      "Batch 63\n",
      "train loss:  1.9375468492507935\n",
      "test loss:  3.466607681910197\n",
      "Batch 64\n",
      "train loss:  1.8369839191436768\n",
      "test loss:  3.4755786259969077\n",
      "Batch 65\n",
      "train loss:  1.8326866626739502\n",
      "test loss:  3.4790491024653116\n",
      "Batch 66\n",
      "train loss:  1.7161736488342285\n",
      "test loss:  3.480085595448812\n",
      "Batch 67\n",
      "train loss:  1.925095558166504\n",
      "test loss:  3.477049406369527\n",
      "Batch 68\n",
      "train loss:  1.7747328281402588\n",
      "test loss:  3.4757867654164634\n",
      "Batch 69\n",
      "train loss:  1.9024648666381836\n",
      "test loss:  3.476334794362386\n",
      "Batch 70\n",
      "train loss:  1.9455748796463013\n",
      "test loss:  3.4709909200668334\n",
      "Batch 71\n",
      "train loss:  2.01993465423584\n",
      "test loss:  3.4717469215393066\n",
      "Batch 72\n",
      "train loss:  1.8534345626831055\n",
      "test loss:  3.475474548339844\n",
      "Batch 73\n",
      "train loss:  1.8648667335510254\n",
      "test loss:  3.4829278707504274\n",
      "Batch 74\n",
      "train loss:  2.0182693004608154\n",
      "test loss:  3.484193778038025\n",
      "Batch 75\n",
      "train loss:  2.0439772605895996\n",
      "test loss:  3.478735852241516\n",
      "Batch 76\n",
      "train loss:  2.063897132873535\n",
      "test loss:  3.4715647300084433\n",
      "Batch 77\n",
      "train loss:  1.7237944602966309\n",
      "test loss:  3.4728585561116536\n",
      "Batch 78\n",
      "train loss:  2.0139527320861816\n",
      "test loss:  3.4768877029418945\n",
      "Batch 79\n",
      "train loss:  2.033386707305908\n",
      "test loss:  3.485864686965942\n",
      "Batch 80\n",
      "train loss:  1.821183204650879\n",
      "test loss:  3.4887460788091023\n",
      "Batch 81\n",
      "train loss:  1.9509371519088745\n",
      "test loss:  3.4864606221516925\n",
      "Batch 82\n",
      "train loss:  2.0023014545440674\n",
      "test loss:  3.4748296976089477\n",
      "Batch 83\n",
      "train loss:  1.745915174484253\n",
      "test loss:  3.469414750734965\n",
      "Batch 84\n",
      "train loss:  1.9688224792480469\n",
      "test loss:  3.470625885327657\n",
      "Batch 85\n",
      "train loss:  1.9299471378326416\n",
      "test loss:  3.4773508071899415\n",
      "Batch 86\n",
      "train loss:  1.9512711763381958\n",
      "test loss:  3.4834683497746783\n",
      "Batch 87\n",
      "train loss:  1.827946662902832\n",
      "test loss:  3.4895307779312135\n",
      "Batch 88\n",
      "train loss:  1.9423675537109375\n",
      "test loss:  3.493990500768026\n",
      "Batch 89\n",
      "train loss:  1.9747226238250732\n",
      "test loss:  3.487436056137085\n",
      "Batch 90\n",
      "train loss:  1.9738860130310059\n",
      "test loss:  3.4829712390899656\n",
      "Batch 91\n",
      "train loss:  1.8657361268997192\n",
      "test loss:  3.4783431847890216\n",
      "Batch 92\n",
      "train loss:  1.9555680751800537\n",
      "test loss:  3.4743042469024656\n",
      "Batch 93\n",
      "train loss:  1.81565260887146\n",
      "test loss:  3.4769399960835776\n",
      "Batch 94\n",
      "train loss:  1.8899459838867188\n",
      "test loss:  3.4852197726567584\n",
      "Batch 95\n",
      "train loss:  1.9751838445663452\n",
      "test loss:  3.4921555121739707\n",
      "Batch 96\n",
      "train loss:  1.9216744899749756\n",
      "test loss:  3.493337710698446\n",
      "Batch 97\n",
      "train loss:  1.8903124332427979\n",
      "test loss:  3.4919551531473796\n",
      "Batch 98\n",
      "train loss:  1.9473029375076294\n",
      "test loss:  3.4853888829549153\n",
      "Batch 99\n",
      "train loss:  2.0376646518707275\n",
      "test loss:  3.483222007751465\n",
      "Batch 100\n",
      "train loss:  1.9965473413467407\n",
      "test loss:  3.489445455869039\n",
      "Batch 101\n",
      "train loss:  1.9509811401367188\n",
      "test loss:  3.4909786621729535\n",
      "Batch 102\n",
      "train loss:  1.837339997291565\n",
      "test loss:  3.4903382062911987\n",
      "Batch 103\n",
      "train loss:  1.9761624336242676\n",
      "test loss:  3.4895747582117718\n",
      "Batch 104\n",
      "train loss:  1.9474992752075195\n",
      "test loss:  3.4874839782714844\n",
      "Batch 105\n",
      "train loss:  2.1153202056884766\n",
      "test loss:  3.492346509297689\n",
      "Batch 106\n",
      "train loss:  1.936873435974121\n",
      "test loss:  3.4993457317352297\n",
      "Batch 107\n",
      "train loss:  2.031245231628418\n",
      "test loss:  3.5039173285166423\n",
      "Batch 108\n",
      "train loss:  1.9159027338027954\n",
      "test loss:  3.504801901181539\n",
      "Batch 109\n",
      "train loss:  2.0047030448913574\n",
      "test loss:  3.4943259159723916\n",
      "Batch 110\n",
      "train loss:  1.9091477394104004\n",
      "test loss:  3.4843504428863525\n",
      "Batch 111\n",
      "train loss:  2.0643720626831055\n",
      "test loss:  3.480619263648987\n",
      "Batch 112\n",
      "train loss:  1.944237470626831\n",
      "test loss:  3.4779447952906293\n",
      "Batch 113\n",
      "train loss:  1.9141103029251099\n",
      "test loss:  3.4718018531799317\n",
      "Batch 114\n",
      "train loss:  1.964062213897705\n",
      "test loss:  3.4746633132298785\n",
      "Batch 115\n",
      "train loss:  2.0579357147216797\n",
      "test loss:  3.4804520924886067\n",
      "Batch 116\n",
      "train loss:  1.9987177848815918\n",
      "test loss:  3.489835031827291\n",
      "Batch 117\n",
      "train loss:  1.907275676727295\n",
      "test loss:  3.4989678064982095\n",
      "Batch 118\n",
      "train loss:  1.9711978435516357\n",
      "test loss:  3.500650231043498\n",
      "Batch 119\n",
      "train loss:  2.0513482093811035\n",
      "test loss:  3.4975355943044026\n",
      "Batch 120\n",
      "train loss:  2.0274085998535156\n",
      "test loss:  3.4941384236017865\n",
      "Batch 121\n",
      "train loss:  1.9882593154907227\n",
      "test loss:  3.485015114148458\n",
      "Batch 122\n",
      "train loss:  1.7973511219024658\n",
      "test loss:  3.4758089780807495\n",
      "Batch 123\n",
      "train loss:  2.0764071941375732\n",
      "test loss:  3.4766528765360514\n",
      "Batch 124\n",
      "train loss:  2.0176398754119873\n",
      "test loss:  3.482100836435954\n",
      "Batch 125\n",
      "train loss:  2.0291271209716797\n",
      "test loss:  3.489184021949768\n",
      "Batch 126\n",
      "train loss:  1.9948687553405762\n",
      "test loss:  3.4881044546763103\n",
      "Batch 127\n",
      "train loss:  2.040482997894287\n",
      "test loss:  3.489463798205058\n",
      "Batch 128\n",
      "train loss:  2.0457897186279297\n",
      "test loss:  3.487155890464783\n",
      "Batch 129\n",
      "train loss:  2.08744740486145\n",
      "test loss:  3.489110326766968\n",
      "Batch 130\n",
      "train loss:  1.951862096786499\n",
      "test loss:  3.489244794845581\n",
      "Batch 131\n",
      "train loss:  1.922189712524414\n",
      "test loss:  3.483092459042867\n",
      "Batch 132\n",
      "train loss:  1.958601713180542\n",
      "test loss:  3.4805769522984824\n",
      "Batch 133\n",
      "train loss:  2.0528793334960938\n",
      "test loss:  3.4813007593154905\n",
      "Batch 134\n",
      "train loss:  2.12276029586792\n",
      "test loss:  3.4853386640548707\n",
      "Batch 135\n",
      "train loss:  2.0544137954711914\n",
      "test loss:  3.491469081242879\n",
      "Batch 136\n",
      "train loss:  1.957747459411621\n",
      "test loss:  3.491254377365112\n",
      "Batch 137\n",
      "train loss:  1.9733213186264038\n",
      "test loss:  3.4896984577178953\n",
      "Batch 138\n",
      "train loss:  1.8855204582214355\n",
      "test loss:  3.4904667854309084\n",
      "Batch 139\n",
      "train loss:  2.034630298614502\n",
      "test loss:  3.4908170382181805\n",
      "Batch 140\n",
      "train loss:  2.0485432147979736\n",
      "test loss:  3.494464373588562\n",
      "Batch 141\n",
      "train loss:  1.8739068508148193\n",
      "test loss:  3.496600890159607\n",
      "Batch 142\n",
      "train loss:  2.050222158432007\n",
      "test loss:  3.496285812060038\n",
      "Batch 143\n",
      "train loss:  2.151022434234619\n",
      "test loss:  3.4927963415781655\n",
      "Batch 144\n",
      "train loss:  1.916965365409851\n",
      "test loss:  3.4958496491114297\n",
      "Batch 145\n",
      "train loss:  2.1042239665985107\n",
      "test loss:  3.497405433654785\n",
      "Batch 146\n",
      "train loss:  2.136873245239258\n",
      "test loss:  3.4977012236913043\n",
      "Batch 147\n",
      "train loss:  2.0412731170654297\n",
      "test loss:  3.494653836886088\n",
      "Batch 148\n",
      "train loss:  1.9879461526870728\n",
      "test loss:  3.4855126619338987\n",
      "Batch 149\n",
      "train loss:  2.1265761852264404\n",
      "test loss:  3.4695507208506267\n",
      "Batch 150\n",
      "train loss:  1.9752002954483032\n",
      "test loss:  3.4645980040232343\n",
      "Batch 151\n",
      "train loss:  1.969923496246338\n",
      "test loss:  3.470694589614868\n",
      "Batch 152\n",
      "train loss:  2.0200533866882324\n",
      "test loss:  3.47636243502299\n",
      "Batch 153\n",
      "train loss:  1.9835643768310547\n",
      "test loss:  3.480189649264018\n",
      "Batch 154\n",
      "train loss:  1.801769495010376\n",
      "test loss:  3.489132523536682\n",
      "Batch 155\n",
      "train loss:  2.0013132095336914\n",
      "test loss:  3.4947935342788696\n",
      "Batch 156\n",
      "train loss:  2.0730247497558594\n",
      "test loss:  3.49363800684611\n",
      "Batch 157\n",
      "train loss:  2.1557741165161133\n",
      "test loss:  3.4946014960606893\n",
      "Batch 158\n",
      "train loss:  1.9500634670257568\n",
      "test loss:  3.4890697558720905\n",
      "Batch 159\n",
      "train loss:  2.095085620880127\n",
      "test loss:  3.484921407699585\n",
      "Batch 160\n",
      "train loss:  2.086811065673828\n",
      "test loss:  3.4878313064575197\n",
      "Batch 161\n",
      "train loss:  2.0691747665405273\n",
      "test loss:  3.489812668164571\n",
      "Batch 162\n",
      "train loss:  2.0107693672180176\n",
      "test loss:  3.4905861218770347\n",
      "Batch 163\n",
      "train loss:  1.9182822704315186\n",
      "test loss:  3.490891631444295\n",
      "Batch 164\n",
      "train loss:  2.0618550777435303\n",
      "test loss:  3.490354625384013\n",
      "Batch 165\n",
      "train loss:  2.0897481441497803\n",
      "test loss:  3.491666873296102\n",
      "Batch 166\n",
      "train loss:  2.01987361907959\n",
      "test loss:  3.498559594154358\n",
      "Batch 167\n",
      "train loss:  2.0122060775756836\n",
      "test loss:  3.492542560895284\n",
      "Batch 168\n",
      "train loss:  2.01861834526062\n",
      "test loss:  3.485486324628194\n",
      "Batch 169\n",
      "train loss:  1.9731533527374268\n",
      "test loss:  3.4874050696690877\n",
      "Batch 170\n",
      "train loss:  2.071228504180908\n",
      "test loss:  3.493768032391866\n",
      "Batch 171\n",
      "train loss:  1.96763277053833\n",
      "test loss:  3.499200526873271\n",
      "Batch 172\n",
      "train loss:  2.027841567993164\n",
      "test loss:  3.4990299542744956\n",
      "Batch 173\n",
      "train loss:  2.061957836151123\n",
      "test loss:  3.5016410748163858\n",
      "Batch 174\n",
      "train loss:  2.137988328933716\n",
      "test loss:  3.4968344926834107\n",
      "Batch 175\n",
      "train loss:  2.213378429412842\n",
      "test loss:  3.486380370457967\n",
      "Batch 176\n",
      "train loss:  1.979099988937378\n",
      "test loss:  3.4839893261591595\n",
      "Batch 177\n",
      "train loss:  1.9168682098388672\n",
      "test loss:  3.490747102101644\n",
      "Batch 178\n",
      "train loss:  2.0246591567993164\n",
      "test loss:  3.4962158679962156\n",
      "Batch 179\n",
      "train loss:  2.1622791290283203\n",
      "test loss:  3.4951099316279093\n",
      "Batch 180\n",
      "train loss:  2.0007128715515137\n",
      "test loss:  3.4865525722503663\n",
      "Batch 181\n",
      "train loss:  2.085313320159912\n",
      "test loss:  3.4782388766606647\n",
      "Batch 182\n",
      "train loss:  2.0359034538269043\n",
      "test loss:  3.4723522901535033\n",
      "Batch 183\n",
      "train loss:  2.147170066833496\n",
      "test loss:  3.473196204503377\n",
      "Batch 184\n",
      "train loss:  2.0845823287963867\n",
      "test loss:  3.478864240646362\n",
      "Batch 185\n",
      "train loss:  2.016467571258545\n",
      "test loss:  3.4903136094411216\n",
      "Batch 186\n",
      "train loss:  2.1238811016082764\n",
      "test loss:  3.4963419437408447\n",
      "Batch 187\n",
      "train loss:  2.032561779022217\n",
      "test loss:  3.4960272312164307\n",
      "Batch 188\n",
      "train loss:  1.9627220630645752\n",
      "test loss:  3.496087678273519\n",
      "Batch 189\n",
      "train loss:  2.0400335788726807\n",
      "test loss:  3.4977277596791585\n",
      "Batch 190\n",
      "train loss:  2.012396812438965\n",
      "test loss:  3.495705246925354\n",
      "Batch 191\n",
      "train loss:  2.095404624938965\n",
      "test loss:  3.492786661783854\n",
      "Batch 192\n",
      "train loss:  1.978533148765564\n",
      "test loss:  3.494910971323649\n",
      "Batch 193\n",
      "train loss:  2.1297173500061035\n",
      "test loss:  3.491112716992696\n",
      "Batch 194\n",
      "train loss:  2.1906917095184326\n",
      "test loss:  3.481438151995341\n",
      "Batch 195\n",
      "train loss:  2.0692479610443115\n",
      "test loss:  3.479308605194092\n",
      "Batch 196\n",
      "train loss:  2.084930419921875\n",
      "test loss:  3.481406315167745\n",
      "Batch 197\n",
      "train loss:  2.0296664237976074\n",
      "test loss:  3.4914908250172934\n",
      "Batch 198\n",
      "train loss:  2.0087151527404785\n",
      "test loss:  3.4974646011988324\n",
      "Batch 199\n",
      "train loss:  2.150564670562744\n",
      "test loss:  3.496287290255229\n",
      "Batch 200\n",
      "train loss:  2.0556509494781494\n",
      "test loss:  3.494852654139201\n",
      "Batch 201\n",
      "train loss:  2.0975522994995117\n",
      "test loss:  3.4937524795532227\n",
      "Batch 202\n",
      "train loss:  2.016118049621582\n",
      "test loss:  3.4962698221206665\n",
      "Batch 203\n",
      "train loss:  2.0614404678344727\n",
      "test loss:  3.4997465054194135\n",
      "Batch 204\n",
      "train loss:  2.233372688293457\n",
      "test loss:  3.4949543555577596\n",
      "Batch 205\n",
      "train loss:  2.2044215202331543\n",
      "test loss:  3.4853691418965655\n",
      "Batch 206\n",
      "train loss:  2.106757640838623\n",
      "test loss:  3.4756332874298095\n",
      "Batch 207\n",
      "train loss:  2.0739004611968994\n",
      "test loss:  3.4724586168924967\n",
      "Batch 208\n",
      "train loss:  2.1518149375915527\n",
      "test loss:  3.476080886522929\n",
      "Batch 209\n",
      "train loss:  2.0385725498199463\n",
      "test loss:  3.484914747873942\n",
      "Batch 210\n",
      "train loss:  2.02028226852417\n",
      "test loss:  3.4910876592000326\n",
      "Batch 211\n",
      "train loss:  1.9724048376083374\n",
      "test loss:  3.495245925585429\n",
      "Batch 212\n",
      "train loss:  2.011690139770508\n",
      "test loss:  3.4941436847050986\n",
      "Batch 213\n",
      "train loss:  1.9836559295654297\n",
      "test loss:  3.485528580347697\n",
      "Batch 214\n",
      "train loss:  2.033635139465332\n",
      "test loss:  3.48188902537028\n",
      "Batch 215\n",
      "train loss:  2.210195541381836\n",
      "test loss:  3.4789299329121905\n",
      "Batch 216\n",
      "train loss:  2.0867514610290527\n",
      "test loss:  3.481344787279765\n",
      "Batch 217\n",
      "train loss:  2.180507183074951\n",
      "test loss:  3.483508412043254\n",
      "Batch 218\n",
      "train loss:  2.121000289916992\n",
      "test loss:  3.485593541463216\n",
      "Batch 219\n",
      "train loss:  2.032294750213623\n",
      "test loss:  3.4924458424250284\n",
      "Batch 220\n",
      "train loss:  2.034390926361084\n",
      "test loss:  3.493300525347392\n",
      "Batch 221\n",
      "train loss:  2.028128147125244\n",
      "test loss:  3.4910643498102822\n",
      "Batch 222\n",
      "train loss:  2.0022268295288086\n",
      "test loss:  3.491509040196737\n",
      "Batch 223\n",
      "train loss:  1.9595974683761597\n",
      "test loss:  3.4924096902211508\n",
      "Batch 224\n",
      "train loss:  2.1011457443237305\n",
      "test loss:  3.4943167606989545\n",
      "Batch 225\n",
      "train loss:  1.9007525444030762\n",
      "test loss:  3.4972467104593914\n",
      "Batch 226\n",
      "train loss:  2.1216745376586914\n",
      "test loss:  3.4985466877619427\n",
      "Batch 227\n",
      "train loss:  2.004343271255493\n",
      "test loss:  3.491720771789551\n",
      "Batch 228\n",
      "train loss:  2.0270137786865234\n",
      "test loss:  3.4903631607691445\n",
      "Batch 229\n",
      "train loss:  2.068758726119995\n",
      "test loss:  3.488847001393636\n",
      "Batch 230\n",
      "train loss:  2.0603930950164795\n",
      "test loss:  3.4867025136947634\n",
      "Batch 231\n",
      "train loss:  2.130829334259033\n",
      "test loss:  3.4863587538401286\n",
      "Batch 232\n",
      "train loss:  2.0555922985076904\n",
      "test loss:  3.4870107809702557\n",
      "Batch 233\n",
      "train loss:  2.124569892883301\n",
      "test loss:  3.4872119188308717\n",
      "Batch 234\n",
      "train loss:  2.1398844718933105\n",
      "test loss:  3.4829121828079224\n",
      "Batch 235\n",
      "train loss:  1.9837734699249268\n",
      "test loss:  3.484030508995056\n",
      "Batch 236\n",
      "train loss:  2.0443317890167236\n",
      "test loss:  3.48807426293691\n",
      "Batch 237\n",
      "train loss:  2.0103092193603516\n",
      "test loss:  3.4871747652689615\n",
      "Batch 238\n",
      "train loss:  2.079165458679199\n",
      "test loss:  3.482415803273519\n",
      "Batch 239\n",
      "train loss:  2.1856045722961426\n",
      "test loss:  3.478937117258708\n",
      "Batch 240\n",
      "train loss:  2.125743865966797\n",
      "test loss:  3.4840986808141072\n",
      "Batch 241\n",
      "train loss:  2.0089497566223145\n",
      "test loss:  3.4884713172912596\n",
      "Batch 242\n",
      "train loss:  2.1708626747131348\n",
      "test loss:  3.486968421936035\n",
      "Batch 243\n",
      "train loss:  2.0547232627868652\n",
      "test loss:  3.482864427566528\n",
      "Batch 244\n",
      "train loss:  2.195173978805542\n",
      "test loss:  3.4797209024429323\n",
      "Batch 245\n",
      "train loss:  2.043022871017456\n",
      "test loss:  3.4793529669443766\n",
      "Batch 246\n",
      "train loss:  2.065230131149292\n",
      "test loss:  3.478536009788513\n",
      "Batch 247\n",
      "train loss:  2.0623059272766113\n",
      "test loss:  3.481071917215983\n",
      "Batch 248\n",
      "train loss:  2.0467963218688965\n",
      "test loss:  3.484822630882263\n",
      "Batch 249\n",
      "train loss:  2.1041135787963867\n",
      "test loss:  3.4815300067265826\n",
      "Batch 250\n",
      "train loss:  2.0340309143066406\n",
      "test loss:  3.480515400568644\n",
      "Batch 251\n",
      "train loss:  2.0536091327667236\n",
      "test loss:  3.4854430119196573\n",
      "Batch 252\n",
      "train loss:  2.040404796600342\n",
      "test loss:  3.4854220231374105\n",
      "Batch 253\n",
      "train loss:  2.161602258682251\n",
      "test loss:  3.4822060108184814\n",
      "Batch 254\n",
      "train loss:  2.174741744995117\n",
      "test loss:  3.481253719329834\n",
      "Batch 255\n",
      "train loss:  2.071002960205078\n",
      "test loss:  3.482987817128499\n",
      "Batch 256\n",
      "train loss:  2.062953472137451\n",
      "test loss:  3.488640292485555\n",
      "Batch 257\n",
      "train loss:  2.187931537628174\n",
      "test loss:  3.4924991210301717\n",
      "Batch 258\n",
      "train loss:  2.1369400024414062\n",
      "test loss:  3.489950489997864\n",
      "Batch 259\n",
      "train loss:  2.057063579559326\n",
      "test loss:  3.487965782483419\n",
      "Batch 260\n",
      "train loss:  2.203911304473877\n",
      "test loss:  3.4806004126866656\n",
      "Batch 261\n",
      "train loss:  2.2665627002716064\n",
      "test loss:  3.473141249020894\n",
      "Batch 262\n",
      "train loss:  1.9199873208999634\n",
      "test loss:  3.471095355351766\n",
      "output tokens:  117\n",
      "generated text:  林黛玉抓住宝玉的手说道：“林姑娘不必问他．他们说起来，    我们也不敢挑了他．他们是个明白的．若不过是叫他们的人，若不敢说出来，他们也是一般，就是这样起来．    \"林黛玉道：“你们也不用说话，他们也不\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 从 start_epoch+1 开始训练\n",
    "next_epoch = start_epoch + 1\n",
    "for epoch in range(next_epoch, next_epoch + EPOCHS):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "     # 每个 epoch 中包含多个 batch，针对每个 batch 进行前向传播、计算损失、反向传播、更新参数\n",
    "    batch_count = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch_count += 1\n",
    "        print(f\"Batch {batch_count}\")\n",
    "\n",
    "\n",
    "        # 每个 batch 训练前，先将模型切换到训练模式\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad() # 清空之前的梯度\n",
    "\n",
    "\n",
    "        # 前向传播\n",
    "        input_ids, label_ids = batch\n",
    "        input_ids, label_ids = input_ids.to(device), label_ids.to(device) # 将输入和标签移动到 GPU 上\n",
    "        # the shape of input_ids: (batch_size, seq_len)\n",
    "        # the shape of label_ids: (batch_size, seq_len)\n",
    "        # input_ids和label_ids都是 2*256，如果 batch 是 2 的话，256 里面是每个 token 的 id，就是在字典里面的位置\n",
    "        output = model(input_ids) # 前向传播，得到模型的输出\n",
    "        # the shape of output: (batch_size, seq_len, vocab_size)\n",
    "        # output里面是已经变成将模型维度的向量转换为 vocab size 维的向量，每个元素是一个 vocab_size 维的向量，每个元素是一个 token id 的概率分布\n",
    "        # 训练的目标就是要这个vocab size 维的向量尽量与 one-hot 一致，也就是每个元素的概率分布与 label_ids 中对应的 token id 一致\n",
    "        # 因为使用了 Causal Attention，输出中的每个位置的vocab size 维的向量都应该和 label_ids 中对应的 token id 一致\n",
    "        # 以 batch_size=2, seq_len=256 为例，output 是一个 2*256*100277 的张量，每个元素是一个 vocab_size 维的向量，每个元素是一个 token id 的概率分布\n",
    "        # label_ids 是一个 2*256 的张量，每个元素是一个 token id\n",
    "        # 要计算损失，需要将 output 和 label_ids 展平为一维向量\n",
    "        # 先将 output 展平为二维，第一维为 batch_size * seq_len，第二维为 vocab_size\n",
    "        # 再将 label_ids 展平为一维，长度为 batch_size * seq_len\n",
    "        # 然后两者就可以对起来，每个位置的 vocab size 维的向量和 label_ids 中对应的 token id 就可以计算损失了\n",
    "        # 每个位置的损失是一个标量，代表这个位置的 token id 的预测概率与真实 token id 的差异\n",
    "        # 对所有位置的损失取平均，就得到了这个 batch 的损失\n",
    "        \n",
    "        ## 直观检验\n",
    "        # #将output转换为具体的文字，与label_ids对比\n",
    "        # predicted_ids = torch.argmax(output, dim=-1) # 取每个位置上概率最大的 token id，得到形状为 (batch_size, seq_len) 的张量\n",
    "        # # 对 predicted_ids 进行 decode，将 predicted_ids 转换为具体一行文字\n",
    "        # predicted_texts = tokenizer.decode_batch(predicted_ids.tolist()) # 得到一个列表，每个元素是一个字符串，代表一个样本的预测文字\n",
    "        # print(\"predicted_texts: \", predicted_texts)\n",
    "        # # 打印label_ids对应的一行文字\n",
    "        # label_texts = tokenizer.decode_batch(label_ids.tolist()) # 得到一个列表，每个元素是一个字符串，代表一个样本的真实文字\n",
    "        # print(\"label_texts: \", label_texts)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), label_ids.view(-1)) \n",
    "        # 计算损失，注意要将输出和标签展平为一维向量, \n",
    "        # output.view(-1, output.size(-1)) 表示将输出张量展平为二维，第一维为 batch_size * seq_len，第二维为 vocab_size\n",
    "        # label_ids.view(-1) 表示将标签张量展平为一维，长度为 batch_size * seq_len\n",
    "\n",
    "        print(\"train loss: \", loss.item())\n",
    "        train_losses.append(loss.item()) # 记录训练损失\n",
    "\n",
    "\n",
    "        # 反向传播和参数更新        \n",
    "        loss.backward() # 计算当前 batch 的梯度\n",
    "        optimizer.step() # 更新模型参数 \n",
    "        #scheduler.step() # 更新学习率\n",
    "        #new learning rate\n",
    "        #print(\"new learning rate: \", scheduler.get_last_lr()[0])   \n",
    "\n",
    "\n",
    "        # 每个 batch 训练完成后，用测试数据集评估模型效果\n",
    "        model.eval() # 切换到评估模式\n",
    "        with torch.no_grad(): # 关闭梯度计算\n",
    "            batch_test_losses = []\n",
    "            for batch in test_dataloader:\n",
    "                input_ids, label_ids = batch \n",
    "                input_ids, label_ids = input_ids.to(device), label_ids.to(device) # 将输入和标签移动到 GPU 上\n",
    "                output = model(input_ids) # 前向传播，得到模型的输出 ，output shape: (batch_size, seq_len, vocab_size)\n",
    "                #output.view(-1, output.size(-1))，把 batch_size 和 seq_len 合并起来，变成一个一维向量，每个元素是一个 vocab_size 维的向量\n",
    "                #label_ids.view(-1)，把 batch_size 和 seq_len 合并起来，变成一个一维向量，每个元素是一个 token id\n",
    "                loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), label_ids.view(-1)) # 计算损失，loss 是一个标量\n",
    "\n",
    "                batch_test_losses.append(loss.item()) # 记录测试损失\n",
    "        #记录这个 batch 的平均损失\n",
    "        test_losses.append(sum(batch_test_losses) / len(batch_test_losses)) # 记录测试损失\n",
    "        print(\"test loss: \", test_losses[-1])   \n",
    "        \n",
    "        # # 检查 loss，如果 loss 小于 1.5，说明模型已经收敛，提前结束训练\n",
    "        # if loss.item() < 1.5:\n",
    "        #     # 保存模型到 checkpoint 文件夹，文件名格式为 model_epoch_{epoch}.pt\n",
    "        #     # 检查是否已经存在 model_epoch_{epoch}.pt 文件\n",
    "        #     if os.path.exists(f\"model_epoch_{epoch}.pt\"):\n",
    "        #         # 如果已经存在，提示用户是否覆盖\n",
    "        #         confirm = input(f\"model_epoch_{epoch}.pt already exists, do you want to overwrite it? (y/n) \")\n",
    "        #         if confirm.lower() != \"y\":\n",
    "        #             # 如果用户不确认，跳过保存\n",
    "        #             continue\n",
    "        #     torch.save(model.state_dict(), f\"model_epoch_{epoch}.pt\")\n",
    "        #     print(\"loss is less than 1.5, stop training\")\n",
    "        #     break\n",
    "        \n",
    "    # 每个 epoch 训练完成后，打印训练损失和测试损失\n",
    "    #print(f\"train loss: {train_losses[-1]:.4f}, test loss: {test_losses[-1]:.4f}\")\n",
    "    \n",
    "    #验证模型在sample_tokens上的表现\n",
    "    model.eval() # 切换到评估模式\n",
    "    with torch.no_grad(): # 关闭梯度计算\n",
    "        output = generate_text(model, sample_tokens, max_new_tokens=100) \n",
    "        print(\"output tokens: \", len(output))\n",
    "        print(\"generated text: \", tokenizer.decode(output)) \n",
    "\n",
    "    # 每个 epoch 训练完成后，保存模型到 checkpoint 文件夹，文件名格式为 model_epoch_{epoch}.pt\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch}.pt\")\n",
    "\n",
    "#展示整个训练过程中，train loss，以及test loss的变化趋势\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(test_losses, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9216c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tokens:  112\n",
      "generated text:  荣府的门前有个小童拿着，    所那边的小丫头拿着一件半点的，也有拿的小丫头，也有半天的．这也有一个的，    所以为这些丫头们的也有八头的，只有这些丫头的丫头都是些的，也有说是那些书的，有这一包\n",
      "output tokens:  113\n",
      "generated text:  宝玉握着一个小苹果品，拿着钱来拍手，拿着一件半碗池花，    看玉钏儿也哄着一声，抿着眼泪抿着，手抿着脚的一般．一时骂道：“好狠儿，你烧的狠的打着\n",
      "output tokens:  111\n",
      "generated text:  林黛玉在房间里拿着钓竿钓髂，    太见黛玉辉手上，躺着眼睛，眼看着眼睛睛直流泪，黛玉钓的钓．宝玉道：“你别动了，别动那鬼淫了．    我也不是这么话。”黛玉点\n",
      "output tokens:  109\n",
      "generated text:  贾琏插了进去，只见凤姐便笑道：“你们只管放在眼里，我也不敢去，我还不敢叫我去．你们不管家，我也不要随便，你们就不依了，我就不依我。”    \n",
      "\n",
      "\n",
      "　　凤姐儿听了，不防下泣，一声笑道：“你们听见，�\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = tokenizer.encode(\"荣府的门前有个小童\")\n",
    "\n",
    "#测试多个\n",
    "samples = [\"荣府的门前有个小童\", \"宝玉握着一个小苹果\", \"林黛玉在房间里\",\"贾琏插了进去\"]\n",
    "\n",
    "#验证模型在sample_tokens上的表现\n",
    "model.eval() # 切换到评估模式\n",
    "with torch.no_grad(): # 关闭梯度计算\n",
    "   for sample in samples:\n",
    "       sample_tokens = tokenizer.encode(sample)\n",
    "       output = generate_text(model, sample_tokens, max_new_tokens=100) \n",
    "       print(\"output tokens: \", len(output))\n",
    "       print(\"generated text: \", tokenizer.decode(output)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "index-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
