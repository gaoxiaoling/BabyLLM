{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b07caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec0966",
   "metadata": {},
   "source": [
    "# 研究pytorch中的 device 已经 GPU 的利用\n",
    "## pytorch中可以通过tensor.to(device)，将这个 tensor 的后续计算放到指定的 device 上进行，但是一个 device 有多个核心，这些核心又如何实现效率最大化呢？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf3cd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000000\u001b[39m):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 矩阵乘法 - 自动分配到多个核心\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 并行执行！\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 元素级运算 - 自动并行\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     d \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m b           \u001b[38;5;66;03m# 并行！\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 测试cpu的多个核心的并行策略\n",
    "# 设置使用的线程数\n",
    "torch.set_num_threads(4)  # 使用4个CPU核心，从电脑的activity monitor看，不止 4 个在工作，似乎这个参数不是和核心一一对应的，\n",
    "\n",
    "\n",
    "# 这些运算会自动并行化\n",
    "a = torch.randn(10000, 10000)\n",
    "b = torch.randn(10000, 10000)\n",
    "\n",
    "\n",
    "for _ in range(1000000):\n",
    "\n",
    "    # 矩阵乘法 - 自动分配到多个核心\n",
    "    c = torch.mm(a, b)  # 并行执行！\n",
    "\n",
    "    # 元素级运算 - 自动并行\n",
    "    d = a + b           # 并行！\n",
    "    e = torch.relu(a)   # 并行！\n",
    "    f = a * b           # 并行！\n",
    "# 注意：实际的并行效率取决于任务的大小和系统的负载情况。    \n",
    "# 输出cpu各个核心的负载情况需要借助系统工具，Python本身无法直接获取。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f185b6e",
   "metadata": {},
   "source": [
    "PyTorch Python API (你写的代码)\n",
    "    ↓\n",
    "PyTorch C++ 后端 (ATen)\n",
    "    ↓\n",
    "计算后端: CUDA / MPS / CPU (OpenMP, MKL等)\n",
    "    ↓\n",
    "硬件: GPU流处理器 / CPU多核心\n",
    "\n",
    "\"\"\"\n",
    "    PyTorch框架层面的\"内置并行\"：\n",
    "    \n",
    "    指的是PyTorch在框架设计时，就为各种运算设计了\n",
    "    自动并行执行的能力，无论后端是什么硬件\n",
    "    \"\"\"\n",
    "    \n",
    "    # 这些运算在PyTorch框架层面就是并行友好的\n",
    "    parallel_operations = {\n",
    "        'element_wise': ['torch.add', 'torch.mul', 'torch.relu'],\n",
    "        'matrix_ops': ['torch.matmul', 'torch.mm', 'torch.bmm'],\n",
    "        'reductions': ['torch.sum', 'torch.mean', 'torch.max'],\n",
    "        'convolution': ['torch.conv1d', 'torch.conv2d', 'torch.conv3d'],\n",
    "        'pooling': ['torch.max_pool2d', 'torch.avg_pool2d']\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "数据并行（Data Parallelism）和张量并行（Tensor Parallelism）是两种不同的并行策略。你的理解基本正确，下面我来详细解释一下，并回答拆、算、合这三个动作是由谁支持的。\n",
    "\n",
    "数据并行（Data Parallelism）\n",
    "概念：将训练数据分成多个小批量，每个GPU上复制相同的模型，然后每个GPU处理一个小批量。每个GPU独立计算梯度，然后通过集合通信（如All-Reduce）来同步梯度。\n",
    "\n",
    "支持：在PyTorch中，可以通过torch.nn.DataParallel（单机多卡）或torch.nn.parallel.DistributedDataParallel（单机或多机多卡）来实现。\n",
    "\n",
    "张量并行（Tensor Parallelism）\n",
    "概念：将模型中的张量（参数）拆分成多个部分，分布到不同的GPU上。每个GPU只拥有张量的一部分，然后通过通信来合并结果。例如，一个大的矩阵乘法可以拆分成多个小矩阵乘法，然后在多个GPU上执行。\n",
    "\n",
    "支持：张量并行通常需要模型本身的支持，即需要将模型中的线性层、注意力层等设计为可拆分的。目前PyTorch本身没有内置的张量并行功能，但是有一些第三方库实现了张量并行，例如：\n",
    "\n",
    "Megatron-LM（由NVIDIA开发，用于训练大型Transformer模型）\n",
    "\n",
    "PyTorch的FSDP（完全分片数据并行）虽然主要是模型并行，但也涉及参数分片。\n",
    "\n",
    "拆、算、合三个动作的支持\n",
    "拆（Split）：通常由用户或模型并行框架来定义如何拆分张量。例如，将一个大矩阵按行或按列拆分。\n",
    "\n",
    "算（Compute）：每个GPU上的计算由PyTorch和相应的后端（CUDA/MPS）来执行。但是，计算的方式需要根据拆分的方式调整，例如，矩阵乘法在拆分后需要相应的局部矩阵乘法和通信来合并结果。\n",
    "\n",
    "合（Merge）：通过集合通信（如All-Reduce、All-Gather等）来合并结果。这通常由通信库（如NCCL）支持，PyTorch的分布式模块（torch.distributed）提供了这些通信原语。\n",
    "\n",
    "总结\n",
    "数据并行：PyTorch内置支持（DataParallel和DistributedDataParallel）。\n",
    "\n",
    "张量并行：目前PyTorch没有直接内置，需要借助第三方库或自己实现。但是PyTorch提供了分布式通信原语（如torch.distributed）和自定义模型拆分的工具，使得实现张量并行成为可能。\n",
    "\n",
    "因此，拆、算、合这三个动作中，拆和合需要由用户或第三方库来设计实现，而算则是由PyTorch和后端（CUDA/MPS）来执行。通信部分由PyTorch的分布式模块和底层的通信库（如NCCL）支持\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef27b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1623])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = torch.sqrt(torch.FloatTensor([10]))\n",
    "scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "index-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
